# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ·±åº¦å¯¹æ¯”æµ‹è¯•æ¡†æ¶
å¯¹æ¯” main.py (åŸå§‹åŒæ­¥ç³»ç»Ÿ) vs main_production.py (æ–°å¼‚æ­¥ç”Ÿäº§ç³»ç»Ÿ)
æ”¯æŒæ‰©å±•çš„æµ‹è¯•é…ç½®å’Œå…¨é¢çš„æ€§èƒ½åˆ†æ
"""

import asyncio
import yaml
import time
import psutil
import pandas as pd
import json
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import tempfile
import threading
import queue
import signal

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ä¸¤ä¸ªç³»ç»Ÿ
from main import VoiceInputSystem as OriginalSystem
from main_production import ProductionVoiceSystem as NewSystem
from audio_capture_v import extract_measurements

@dataclass
class TestResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    test_case: str
    input_text: str
    expected_values: List[float]
    actual_values: List[float]
    accuracy: float
    response_time: float
    cpu_usage: float
    memory_usage: float
    excel_output: Optional[str]
    print_output: Optional[str]  # æ–°å¢ï¼šPrintåŠŸèƒ½è¾“å‡º
    errors: List[str]
    timestamp: str
    system_type: str  # 'original' æˆ– 'new'
    test_category: str  # æµ‹è¯•åˆ†ç±»

@dataclass
class SystemMetrics:
    """ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
    avg_response_time: float
    avg_cpu_usage: float
    avg_memory_usage: float
    total_tests: int
    passed_tests: int
    accuracy_rate: float
    excel_consistency: float
    print_function_accuracy: float  # PrintåŠŸèƒ½å‡†ç¡®åº¦

class EnhancedComparisonFramework:
    """å¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•æ¡†æ¶"""

    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.results: Dict[str, List[TestResult]] = {
            'original': [],
            'new': []
        }
        self.logger = self._setup_logging()
        self._stop_event = threading.Event()
        self._resource_monitor_thread = None
        self._resource_data = []

        # æ–°å¢ï¼šæµ‹è¯•è¿›åº¦è·Ÿè¸ª
        self._total_tests = 0
        self._completed_tests = 0
        self._current_suite = ""
        self._current_test = 0
        self._suite_tests = 0
        self._test_suites = [
            'voice_data',
            'voice_data_complex',
            'voice_data_edge_cases',
            'voice_data_error_recovery',
            'voice_data_with_commands',
            'voice_data_performance',
            'voice_data_tts_interference',
            'keyboard_interference_tests',
            'voice_command_chaos_tests',
            'comprehensive_stress_tests',
            'non_numeric_input_tests',
            'mixed_language_tests'
        ]

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}

    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        logger = logging.getLogger('enhanced_comparison')
        logger.setLevel(logging.INFO)

        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path("logs/comparison/enhanced")
        log_dir.mkdir(parents=True, exist_ok=True)

        # æ–‡ä»¶å¤„ç†å™¨
        fh = logging.FileHandler(
            log_dir / f"enhanced_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
            encoding='utf-8'
        )
        fh.setLevel(logging.INFO)

        # æ§åˆ¶å°å¤„ç†å™¨
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)

        # æ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)

        logger.addHandler(fh)
        logger.addHandler(ch)

        return logger

    def _start_resource_monitoring(self):
        """å¯åŠ¨èµ„æºç›‘æ§"""
        def monitor_resources():
            while not self._stop_event.is_set():
                try:
                    cpu_percent = psutil.cpu_percent(interval=0.1)
                    memory_info = psutil.virtual_memory()

                    self._resource_data.append({
                        'timestamp': time.time(),
                        'cpu_percent': cpu_percent,
                        'memory_percent': memory_info.percent,
                        'memory_mb': memory_info.used / 1024 / 1024
                    })

                    time.sleep(0.1)  # 100msé‡‡æ ·é—´éš”
                except Exception as e:
                    self.logger.error(f"èµ„æºç›‘æ§é”™è¯¯: {e}")

        self._resource_monitor_thread = threading.Thread(target=monitor_resources, daemon=True)
        self._resource_monitor_thread.start()

    def _stop_resource_monitoring(self) -> Dict[str, float]:
        """åœæ­¢èµ„æºç›‘æ§å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯"""
        self._stop_event.set()
        if self._resource_monitor_thread:
            self._resource_monitor_thread.join(timeout=2)

        if not self._resource_data:
            return {'avg_cpu': 0.0, 'avg_memory': 0.0, 'peak_cpu': 0.0, 'peak_memory': 0.0}

        cpu_values = [d['cpu_percent'] for d in self._resource_data]
        memory_values = [d['memory_percent'] for d in self._resource_data]

        return {
            'avg_cpu': sum(cpu_values) / len(cpu_values),
            'avg_memory': sum(memory_values) / len(memory_values),
            'peak_cpu': max(cpu_values),
            'peak_memory': max(memory_values)
        }

    def _calculate_accuracy(self, expected: List[float], actual: List[float]) -> float:
        """è®¡ç®—è¯†åˆ«å‡†ç¡®åº¦"""
        if not expected and not actual:
            return 1.0
        if not expected or not actual:
            return 0.0
        if len(expected) != len(actual):
            return 0.0

        matches = 0
        for exp, act in zip(expected, actual):
            if abs(exp - act) < 0.001:  # å…è®¸å°çš„æµ®ç‚¹è¯¯å·®
                matches += 1

        return matches / len(expected)

    def _handle_print_function(self, text: str) -> Tuple[bool, Optional[str]]:
        """å¤„ç†PrintåŠŸèƒ½
        è¿”å›: (æ˜¯å¦æ˜¯Printå‘½ä»¤, Printå†…å®¹)
        """
        text_lower = text.lower().strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯Printå‘½ä»¤
        if text_lower.startswith('print '):
            print_content = text[6:].strip()  # ç§»é™¤"Print "å‰ç¼€
            return True, print_content
        elif 'print' in text_lower:
            # å¤„ç†åŒ…å«printä½†ä¸ä»¥printå¼€å¤´çš„æƒ…å†µ
            parts = text.split('print', 1)
            if len(parts) == 2:
                return True, parts[1].strip()

        return False, None

    def _run_original_system_test(self, test_data: Dict) -> TestResult:
        """è¿è¡ŒåŸå§‹ç³»ç»Ÿæµ‹è¯•"""
        start_time = time.time()

        try:
            # å¯åŠ¨èµ„æºç›‘æ§
            self._resource_data.clear()
            self._stop_event.clear()
            self._start_resource_monitoring()

            # å¤„ç†æ–‡æœ¬
            input_text = test_data['text']

            # æ£€æŸ¥PrintåŠŸèƒ½
            is_print, print_content = self._handle_print_function(input_text)
            expected_print = test_data.get('expected_print')

            if is_print:
                # PrintåŠŸèƒ½æµ‹è¯•
                actual_print = print_content
                actual_values = []
                accuracy = 1.0 if (expected_print and actual_print == expected_print) else 0.0
            else:
                # æ•°å­—æå–æµ‹è¯•
                actual_values = extract_measurements(input_text)
                expected_values = test_data.get('values', [])
                accuracy = self._calculate_accuracy(expected_values, actual_values)
                actual_print = None

            response_time = time.time() - start_time

            # åœæ­¢èµ„æºç›‘æ§
            resource_stats = self._stop_resource_monitoring()

            return TestResult(
                test_case=test_data.get('description', 'Unknown'),
                input_text=input_text,
                expected_values=test_data.get('values', []),
                actual_values=actual_values,
                accuracy=accuracy,
                response_time=response_time,
                cpu_usage=resource_stats['avg_cpu'],
                memory_usage=resource_stats['avg_memory'],
                excel_output=None,  # åç»­æ·»åŠ Excelæµ‹è¯•
                print_output=actual_print,
                errors=[],
                timestamp=datetime.now().isoformat(),
                system_type='original',
                test_category=self._get_test_category(test_data)
            )

        except Exception as e:
            response_time = time.time() - start_time
            resource_stats = self._stop_resource_monitoring()

            self.logger.error(f"åŸå§‹ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
            return TestResult(
                test_case=test_data.get('description', 'Unknown'),
                input_text=test_data.get('text', ''),
                expected_values=test_data.get('values', []),
                actual_values=[],
                accuracy=0.0,
                response_time=response_time,
                cpu_usage=resource_stats['avg_cpu'],
                memory_usage=resource_stats['avg_memory'],
                excel_output=None,
                print_output=None,
                errors=[str(e)],
                timestamp=datetime.now().isoformat(),
                system_type='original',
                test_category=self._get_test_category(test_data)
            )

    async def _run_new_system_test(self, test_data: Dict) -> TestResult:
        """è¿è¡Œæ–°ç³»ç»Ÿæµ‹è¯•"""
        start_time = time.time()

        try:
            # å¯åŠ¨èµ„æºç›‘æ§
            self._resource_data.clear()
            self._stop_event.clear()
            self._start_resource_monitoring()

            # å¤„ç†æ–‡æœ¬
            input_text = test_data['text']

            # æ£€æŸ¥PrintåŠŸèƒ½
            is_print, print_content = self._handle_print_function(input_text)
            expected_print = test_data.get('expected_print')

            if is_print:
                # PrintåŠŸèƒ½æµ‹è¯•
                actual_print = print_content
                actual_values = []
                accuracy = 1.0 if (expected_print and actual_print == expected_print) else 0.0
            else:
                # æ•°å­—æå–æµ‹è¯•
                actual_values = extract_measurements(input_text)
                expected_values = test_data.get('values', [])
                accuracy = self._calculate_accuracy(expected_values, actual_values)
                actual_print = None

            response_time = time.time() - start_time

            # åœæ­¢èµ„æºç›‘æ§
            resource_stats = self._stop_resource_monitoring()

            return TestResult(
                test_case=test_data.get('description', 'Unknown'),
                input_text=input_text,
                expected_values=test_data.get('values', []),
                actual_values=actual_values,
                accuracy=accuracy,
                response_time=response_time,
                cpu_usage=resource_stats['avg_cpu'],
                memory_usage=resource_stats['avg_memory'],
                excel_output=None,
                print_output=actual_print,
                errors=[],
                timestamp=datetime.now().isoformat(),
                system_type='new',
                test_category=self._get_test_category(test_data)
            )

        except Exception as e:
            response_time = time.time() - start_time
            resource_stats = self._stop_resource_monitoring()

            self.logger.error(f"æ–°ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
            return TestResult(
                test_case=test_data.get('description', 'Unknown'),
                input_text=test_data.get('text', ''),
                expected_values=test_data.get('values', []),
                actual_values=[],
                accuracy=0.0,
                response_time=response_time,
                cpu_usage=resource_stats['avg_cpu'],
                memory_usage=resource_stats['avg_memory'],
                excel_output=None,
                print_output=None,
                errors=[str(e)],
                timestamp=datetime.now().isoformat(),
                system_type='new',
                test_category=self._get_test_category(test_data)
            )

    def _calculate_total_tests(self, test_suites: List[str] = None) -> int:
        """è®¡ç®—æ€»æµ‹è¯•æ•°é‡"""
        if test_suites is None:
            test_suites = self._test_suites

        total = 0
        for suite_name in test_suites:
            if suite_name in self.config:
                total += len(self.config[suite_name])
        return total

    def get_test_progress(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•è¿›åº¦ä¿¡æ¯"""
        return {
            'total_tests': self._total_tests,
            'completed_tests': self._completed_tests,
            'progress_percentage': (self._completed_tests / self._total_tests * 100) if self._total_tests > 0 else 0,
            'current_suite': self._current_suite,
            'current_test': self._current_test,
            'suite_tests': self._suite_tests,
            'remaining_tests': self._total_tests - self._completed_tests
        }

    def print_test_progress(self):
        """æ‰“å°æµ‹è¯•è¿›åº¦"""
        progress = self.get_test_progress()
        print(f"\nğŸ“Š æµ‹è¯•è¿›åº¦æŠ¥å‘Š")
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"æ€»æµ‹è¯•æ•°é‡: {progress['total_tests']}")
        print(f"å·²å®Œæˆæµ‹è¯•: {progress['completed_tests']}")
        print(f"å‰©ä½™æµ‹è¯•: {progress['remaining_tests']}")
        print(f"å®Œæˆè¿›åº¦: {progress['progress_percentage']:.1f}%")
        if progress['current_suite']:
            print(f"å½“å‰å¥—ä»¶: {progress['current_suite']}")
            print(f"å¥—ä»¶è¿›åº¦: {progress['current_test']}/{progress['suite_tests']}")
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    def _get_test_category(self, test_data: Dict) -> str:
        """è·å–æµ‹è¯•åˆ†ç±»"""
        # æ ¹æ®æµ‹è¯•æ•°æ®é”®ååˆ¤æ–­åˆ†ç±»
        if 'tts_' in str(test_data):
            return 'tts_interference'
        elif 'keyboard_' in str(test_data):
            return 'keyboard_interference'
        elif 'voice_command' in str(test_data):
            return 'voice_command_chaos'
        elif 'comprehensive' in str(test_data):
            return 'comprehensive_stress'
        elif 'print_function' in str(test_data):
            return 'print_function'
        elif 'mixed_language' in str(test_data):
            return 'mixed_language'
        elif 'performance' in str(test_data):
            return 'performance'
        elif 'complex' in str(test_data):
            return 'complex'
        elif 'edge_cases' in str(test_data):
            return 'edge_cases'
        elif 'error_recovery' in str(test_data):
            return 'error_recovery'
        else:
            return 'basic'

    async def _run_test_suite(self, system_type: str, test_suite_name: str) -> List[TestResult]:
        """è¿è¡ŒæŒ‡å®šçš„æµ‹è¯•å¥—ä»¶"""
        suite_start_time = time.time()
        self.logger.info(f"è¿è¡Œ {system_type} ç³»ç»Ÿçš„ {test_suite_name} æµ‹è¯•å¥—ä»¶")

        # æ›´æ–°å½“å‰å¥—ä»¶ä¿¡æ¯
        self._current_suite = test_suite_name
        test_data = self.config.get(test_suite_name, [])
        self._suite_tests = len(test_data)
        results = []

        for i, test_case in enumerate(test_data):
            # æ›´æ–°å½“å‰æµ‹è¯•è¿›åº¦
            self._current_test = i + 1
            self.logger.info(f"æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_data)}: {test_case.get('description', 'Unknown')}")

            # æ¯5ä¸ªæµ‹è¯•æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 5 == 0 or (i + 1) == len(test_data):
                self.print_test_progress()

            if system_type == 'original':
                result = self._run_original_system_test(test_case)
            else:
                # æ–°ç³»ç»Ÿæµ‹è¯•ä½¿ç”¨awaitç›´æ¥è°ƒç”¨
                result = await self._run_new_system_test(test_case)

            results.append(result)
            self._completed_tests += 1  # å¢åŠ å·²å®Œæˆè®¡æ•°

            # å»¶è¿Ÿæ¨¡æ‹ŸçœŸå®è¾“å…¥é—´éš”
            delay = test_case.get('delay', 1)
            await asyncio.sleep(delay)

        # è®°å½•å¥—ä»¶å®Œæˆæ—¶é—´
        suite_end_time = time.time()
        suite_duration = suite_end_time - suite_start_time
        self.logger.info(f"{system_type} ç³»ç»Ÿ {test_suite_name} å¥—ä»¶å®Œæˆ - è€—æ—¶: {suite_duration:.2f}ç§’, æµ‹è¯•æ•°é‡: {len(test_data)}")

        return results

    async def run_enhanced_comparison(self, test_suites: List[str] = None) -> Dict[str, Any]:
        """è¿è¡Œå¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•"""
        if test_suites is None:
            test_suites = [
                'voice_data',
                'voice_data_complex',
                'voice_data_edge_cases',
                'voice_data_error_recovery',
                'voice_data_with_commands',
                'voice_data_performance',
                'voice_data_tts_interference',
                'keyboard_interference_tests',
                'voice_command_chaos_tests',
                'comprehensive_stress_tests',
                'non_numeric_input_tests',
                'mixed_language_tests'
            ]

        self.logger.info("å¼€å§‹å¢å¼ºç‰ˆæ·±åº¦å¯¹æ¯”æµ‹è¯•")

        # ä¸ºæ¯ä¸ªç³»ç»Ÿè¿è¡Œæµ‹è¯•
        for system_type in ['original', 'new']:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"æµ‹è¯• {system_type.upper()} ç³»ç»Ÿ")
            self.logger.info(f"{'='*60}")

            for suite_name in test_suites:
                if suite_name in self.config:
                    suite_results = await self._run_test_suite(system_type, suite_name)
                    self.results[system_type].extend(suite_results)

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        return self._generate_enhanced_comparison_report()

    def _generate_enhanced_comparison_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆå¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š")

        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {},
            'detailed_results': {},
            'test_categories': {},
            'recommendations': [],
            'performance_analysis': {},
            'feature_comparison': {}
        }

        # åˆ†ææ¯ä¸ªç³»ç»Ÿçš„ç»“æœ
        for system_type in ['original', 'new']:
            results = self.results[system_type]
            if not results:
                continue

            # æŒ‰æµ‹è¯•åˆ†ç±»ç»Ÿè®¡
            categories = {}
            for result in results:
                category = result.test_category
                if category not in categories:
                    categories[category] = []
                categories[category].append(result)

            # è®¡ç®—ç³»ç»ŸæŒ‡æ ‡
            metrics = SystemMetrics(
                avg_response_time=sum(r.response_time for r in results) / len(results),
                avg_cpu_usage=sum(r.cpu_usage for r in results) / len(results),
                avg_memory_usage=sum(r.memory_usage for r in results) / len(results),
                total_tests=len(results),
                passed_tests=sum(1 for r in results if r.accuracy > 0.9),
                accuracy_rate=sum(r.accuracy for r in results) / len(results),
                excel_consistency=0.0,  # éœ€è¦ä¸¤ä¸ªç³»ç»Ÿéƒ½æœ‰ç»“æœæ‰èƒ½è®¡ç®—
                print_function_accuracy=sum(1 for r in results if r.print_output is not None) / len(results) if results else 0.0
            )

            report['summary'][system_type] = asdict(metrics)
            report['detailed_results'][system_type] = [asdict(r) for r in results]
            report['test_categories'][system_type] = {
                cat: {
                    'total': len(cat_results),
                    'avg_accuracy': sum(r.accuracy for r in cat_results) / len(cat_results),
                    'avg_response_time': sum(r.response_time for r in cat_results) / len(cat_results)
                }
                for cat, cat_results in categories.items()
            }

        # ç”Ÿæˆå»ºè®®
        report['recommendations'] = self._generate_enhanced_recommendations(report['summary'])

        # æ€§èƒ½åˆ†æ
        report['performance_analysis'] = self._analyze_performance(report['summary'])

        # åŠŸèƒ½å¯¹æ¯”
        report['feature_comparison'] = self._compare_features(report['summary'])

        # ä¿å­˜æŠ¥å‘Š
        self._save_enhanced_report(report)

        return report

    def _generate_enhanced_recommendations(self, summary: Dict) -> List[str]:
        """ç”Ÿæˆå¢å¼ºç‰ˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        if 'original' in summary and 'new' in summary:
            orig = summary['original']
            new = summary['new']

            # æ€§èƒ½å¯¹æ¯”
            if new['avg_response_time'] < orig['avg_response_time']:
                improvement = (orig['avg_response_time'] - new['avg_response_time']) / orig['avg_response_time'] * 100
                recommendations.append(f"âœ… æ–°ç³»ç»Ÿå“åº”æ—¶é—´æå‡ {improvement:.1f}%")
            else:
                degradation = (new['avg_response_time'] - orig['avg_response_time']) / orig['avg_response_time'] * 100
                recommendations.append(f"âš ï¸ æ–°ç³»ç»Ÿå“åº”æ—¶é—´ä¸‹é™ {degradation:.1f}%ï¼Œå»ºè®®ä¼˜åŒ–å¼‚æ­¥å¤„ç†é€»è¾‘")

            # å‡†ç¡®åº¦å¯¹æ¯”
            if new['accuracy_rate'] > orig['accuracy_rate']:
                improvement = (new['accuracy_rate'] - orig['accuracy_rate']) * 100
                recommendations.append(f"âœ… æ–°ç³»ç»Ÿå‡†ç¡®åº¦æå‡ {improvement:.1f}%")
            else:
                degradation = (orig['accuracy_rate'] - new['accuracy_rate']) * 100
                recommendations.append(f"âš ï¸ æ–°ç³»ç»Ÿå‡†ç¡®åº¦ä¸‹é™ {degradation:.1f}%ï¼Œå»ºè®®æ£€æŸ¥æ•°å­—æå–é€»è¾‘")

            # PrintåŠŸèƒ½å¯¹æ¯”
            if new['print_function_accuracy'] > orig['print_function_accuracy']:
                recommendations.append(f"âœ… æ–°ç³»ç»ŸPrintåŠŸèƒ½å®ç°æ›´å¥½")
            elif new['print_function_accuracy'] < orig['print_function_accuracy']:
                recommendations.append(f"âš ï¸ æ–°ç³»ç»ŸPrintåŠŸèƒ½éœ€è¦æ”¹è¿›")

            # èµ„æºä½¿ç”¨å¯¹æ¯”
            if new['avg_cpu_usage'] < orig['avg_cpu_usage']:
                recommendations.append("âœ… æ–°ç³»ç»ŸCPUä½¿ç”¨ç‡æ›´ä½ï¼Œæ€§èƒ½æ›´ä¼˜")
            if new['avg_memory_usage'] < orig['avg_memory_usage']:
                recommendations.append("âœ… æ–°ç³»ç»Ÿå†…å­˜ä½¿ç”¨æ›´å°‘ï¼Œèµ„æºæ•ˆç‡æ›´é«˜")

        return recommendations

    def _analyze_performance(self, summary: Dict) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ•°æ®"""
        analysis = {}

        if 'original' in summary and 'new' in summary:
            orig = summary['original']
            new = summary['new']

            analysis['response_time'] = {
                'original': orig['avg_response_time'],
                'new': new['avg_response_time'],
                'improvement': (orig['avg_response_time'] - new['avg_response_time']) / orig['avg_response_time'] * 100
            }

            analysis['resource_usage'] = {
                'cpu_improvement': (orig['avg_cpu_usage'] - new['avg_cpu_usage']) / orig['avg_cpu_usage'] * 100,
                'memory_improvement': (orig['avg_memory_usage'] - new['avg_memory_usage']) / orig['avg_memory_usage'] * 100
            }

            analysis['overall_score'] = (
                analysis['response_time']['improvement'] * 0.4 +
                analysis['resource_usage']['cpu_improvement'] * 0.3 +
                analysis['resource_usage']['memory_improvement'] * 0.3
            )

        return analysis

    def _compare_features(self, summary: Dict) -> Dict[str, Any]:
        """åŠŸèƒ½å¯¹æ¯”åˆ†æ"""
        features = {}

        if 'original' in summary and 'new' in summary:
            orig = summary['original']
            new = summary['new']

            features['accuracy'] = {
                'original': orig['accuracy_rate'],
                'new': new['accuracy_rate'],
                'winner': 'new' if new['accuracy_rate'] > orig['accuracy_rate'] else 'original'
            }

            features['performance'] = {
                'original': orig['avg_response_time'],
                'new': new['avg_response_time'],
                'winner': 'new' if new['avg_response_time'] < orig['avg_response_time'] else 'original'
            }

            features['print_function'] = {
                'original': orig.get('print_function_accuracy', 0),
                'new': new.get('print_function_accuracy', 0),
                'winner': 'new' if new.get('print_function_accuracy', 0) > orig.get('print_function_accuracy', 0) else 'original'
            }

        return features

    def _save_enhanced_report(self, report: Dict[str, Any]):
        """ä¿å­˜å¢å¼ºç‰ˆæµ‹è¯•æŠ¥å‘Š"""
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = Path("reports/comparison/enhanced")
        report_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜JSONæŠ¥å‘Š
        json_report_file = report_dir / f"enhanced_comparison_report_{timestamp}.json"
        with open(json_report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # ç”Ÿæˆè¯¦ç»†çš„MarkdownæŠ¥å‘Š
        md_report_file = report_dir / f"enhanced_comparison_report_{timestamp}.md"
        md_content = self._generate_markdown_report(report)
        with open(md_report_file, 'w', encoding='utf-8') as f:
            f.write(md_content)

        # ç”ŸæˆCSVæ•°æ®æ–‡ä»¶
        csv_data = self._generate_csv_data(report)
        csv_file = report_dir / f"enhanced_comparison_data_{timestamp}.csv"
        csv_data.to_csv(csv_file, index=False, encoding='utf-8')

        self.logger.info(f"å¢å¼ºç‰ˆæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
        self.logger.info(f"  JSONæŠ¥å‘Š: {json_report_file}")
        self.logger.info(f"  MarkdownæŠ¥å‘Š: {md_report_file}")
        self.logger.info(f"  CSVæ•°æ®: {csv_file}")

    def _generate_markdown_report(self, report: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        lines = []

        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        lines.extend([
            "# è¯­éŸ³è¾“å…¥ç³»ç»Ÿå¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š",
            f"**ç”Ÿæˆæ—¶é—´**: {report['timestamp']}",
            f"**å¯¹æ¯”ç³»ç»Ÿ**: åŸå§‹åŒæ­¥ç³»ç»Ÿ vs æ–°å¼‚æ­¥ç”Ÿäº§ç³»ç»Ÿ",
            "",
            "## ğŸ“Š æµ‹è¯•æ¦‚è¿°",
            "",
            "æœ¬æ¬¡æµ‹è¯•å¯¹æ¯”äº†åŸå§‹åŒæ­¥ç³»ç»Ÿå’Œæ–°å¼‚æ­¥ç”Ÿäº§ç³»ç»Ÿåœ¨ä»¥ä¸‹æ–¹é¢çš„è¡¨ç°ï¼š",
            "- âœ… æ•°å­—è¯†åˆ«å‡†ç¡®åº¦",
            "- âœ… ç³»ç»Ÿå“åº”æ€§èƒ½",
            "- âœ… èµ„æºä½¿ç”¨æ•ˆç‡",
            "- âœ… PrintåŠŸèƒ½æ”¯æŒ",
            "- âœ… å„ç§å¹²æ‰°åœºæ™¯ä¸‹çš„ç¨³å®šæ€§",
            "",
            "## ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”",
            ""
        ])

        # æ ¸å¿ƒæŒ‡æ ‡è¡¨æ ¼
        lines.extend([
            "| æŒ‡æ ‡ | åŸå§‹ç³»ç»Ÿ | æ–°ç³»ç»Ÿ | å˜åŒ– |",
            "|------|----------|--------|------|"
        ])

        for system_type, metrics in report['summary'].items():
            if system_type == 'original':
                orig_metrics = metrics
            else:
                new_metrics = metrics

        if 'original' in report['summary'] and 'new' in report['summary']:
            orig = report['summary']['original']
            new = report['summary']['new']

            # å‡†ç¡®åº¦
            accuracy_change = (new['accuracy_rate'] - orig['accuracy_rate']) * 100
            lines.append(f"| å¹³å‡å‡†ç¡®åº¦ | {orig['accuracy_rate']:.3f} | {new['accuracy_rate']:.3f} | {'â†—ï¸ +' if accuracy_change > 0 else 'â†˜ï¸ '}{accuracy_change:.1f}% |")

            # å“åº”æ—¶é—´
            time_change = (orig['avg_response_time'] - new['avg_response_time']) / orig['avg_response_time'] * 100
            lines.append(f"| å¹³å‡å“åº”æ—¶é—´ | {orig['avg_response_time']:.3f}s | {new['avg_response_time']:.3f}s | {'â†—ï¸ +' if time_change > 0 else 'â†˜ï¸ '}{abs(time_change):.1f}% |")

            # CPUä½¿ç”¨ç‡
            cpu_change = (orig['avg_cpu_usage'] - new['avg_cpu_usage']) / orig['avg_cpu_usage'] * 100
            lines.append(f"| å¹³å‡CPUä½¿ç”¨ç‡ | {orig['avg_cpu_usage']:.1f}% | {new['avg_cpu_usage']:.1f}% | {'â†—ï¸ +' if cpu_change > 0 else 'â†˜ï¸ '}{abs(cpu_change):.1f}% |")

            # å†…å­˜ä½¿ç”¨ç‡
            mem_change = (orig['avg_memory_usage'] - new['avg_memory_usage']) / orig['avg_memory_usage'] * 100
            lines.append(f"| å¹³å‡å†…å­˜ä½¿ç”¨ç‡ | {orig['avg_memory_usage']:.1f}% | {new['avg_memory_usage']:.1f}% | {'â†—ï¸ +' if mem_change > 0 else 'â†˜ï¸ '}{abs(mem_change):.1f}% |")

        lines.extend(["", "## ğŸ¯ åˆ†ç±»æµ‹è¯•ç»“æœ", ""])

        # åˆ†ç±»æµ‹è¯•ç»“æœ
        for system_type, categories in report['test_categories'].items():
            lines.extend([
                f"### {system_type.upper()} ç³»ç»Ÿ",
                "",
                "| æµ‹è¯•åˆ†ç±» | æµ‹è¯•æ•°é‡ | å¹³å‡å‡†ç¡®åº¦ | å¹³å‡å“åº”æ—¶é—´ |",
                "|----------|----------|------------|--------------|"
            ])

            for category, stats in categories.items():
                lines.append(f"| {category} | {stats['total']} | {stats['avg_accuracy']:.3f} | {stats['avg_response_time']:.3f}s |")

            lines.append("")

        # æ€§èƒ½åˆ†æ
        if 'performance_analysis' in report and report['performance_analysis']:
            analysis = report['performance_analysis']
            lines.extend([
                "## âš¡ æ€§èƒ½åˆ†æ",
                "",
                "### ğŸ“Š æ•´ä½“æ€§èƒ½è¯„åˆ†",
                f"**ç»¼åˆæ€§èƒ½æå‡**: {analysis.get('overall_score', 0):.1f}%",
                "",
                "### ğŸ” è¯¦ç»†åˆ†æ",
                f"- **å“åº”æ—¶é—´å˜åŒ–**: {analysis['response_time']['improvement']:.1f}%",
                f"- **CPUä½¿ç”¨ä¼˜åŒ–**: {analysis['resource_usage']['cpu_improvement']:.1f}%",
                f"- **å†…å­˜ä½¿ç”¨ä¼˜åŒ–**: {analysis['resource_usage']['memory_improvement']:.1f}%",
                ""
            ])

        # åŠŸèƒ½å¯¹æ¯”
        if 'feature_comparison' in report and report['feature_comparison']:
            features = report['feature_comparison']
            lines.extend([
                "## ğŸ† åŠŸèƒ½å¯¹æ¯”ç»“æœ",
                "",
                "| åŠŸèƒ½ | è·èƒœæ–¹ | è¯´æ˜ |",
                "|------|--------|------|"
            ])

            for feature, comparison in features.items():
                winner = comparison['winner']
                if feature == 'accuracy':
                    lines.append(f"| æ•°å­—è¯†åˆ«å‡†ç¡®åº¦ | {winner.upper()} | æ–°ç³»ç»Ÿ: {comparison['new']:.3f}, åŸå§‹ç³»ç»Ÿ: {comparison['original']:.3f} |")
                elif feature == 'performance':
                    lines.append(f"| ç³»ç»Ÿå“åº”æ€§èƒ½ | {winner.upper()} | æ–°ç³»ç»Ÿ: {comparison['new']:.3f}s, åŸå§‹ç³»ç»Ÿ: {comparison['original']:.3f}s |")
                elif feature == 'print_function':
                    lines.append(f"| PrintåŠŸèƒ½æ”¯æŒ | {winner.upper()} | æ–°ç³»ç»Ÿ: {comparison['new']:.3f}, åŸå§‹ç³»ç»Ÿ: {comparison['original']:.3f} |")

        # æ”¹è¿›å»ºè®®
        if 'recommendations' in report and report['recommendations']:
            lines.extend([
                "", "## ğŸ’¡ æ”¹è¿›å»ºè®®", ""
            ])
            for i, rec in enumerate(report['recommendations'], 1):
                lines.append(f"{i}. {rec}")

        lines.extend([
            "",
            "---",
            f"*æŠ¥å‘Šç”±å¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•æ¡†æ¶ç”Ÿæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        ])

        return '\n'.join(lines)

    def _generate_csv_data(self, report: Dict[str, Any]) -> pd.DataFrame:
        """ç”ŸæˆCSVæ•°æ®"""
        data = []

        for system_type, results in report['detailed_results'].items():
            for result in results:
                data.append({
                    'system_type': system_type,
                    'test_case': result['test_case'],
                    'input_text': result['input_text'],
                    'expected_values': str(result['expected_values']),
                    'actual_values': str(result['actual_values']),
                    'accuracy': result['accuracy'],
                    'response_time': result['response_time'],
                    'cpu_usage': result['cpu_usage'],
                    'memory_usage': result['memory_usage'],
                    'test_category': result['test_category'],
                    'has_errors': len(result['errors']) > 0
                })

        return pd.DataFrame(data)

    async def run_enhanced_comparison_for_system(self, system_type: str, test_suites: List[str] = None) -> Dict[str, Any]:
        """è¿è¡ŒæŒ‡å®šç³»ç»Ÿçš„å¢å¼ºç‰ˆæµ‹è¯•"""
        if test_suites is None:
            test_suites = self._test_suites

        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
        self._total_tests = self._calculate_total_tests(test_suites)
        self._completed_tests = 0

        self.logger.info(f"å¼€å§‹{system_type}ç³»ç»Ÿä¸“é¡¹æµ‹è¯•")
        print(f"\nğŸš€ å¼€å§‹{system_type}ç³»ç»Ÿä¸“é¡¹æµ‹è¯•")
        print(f"æ€»æµ‹è¯•æ•°é‡: {self._total_tests} ä¸ª")

        # åªä¸ºæŒ‡å®šç³»ç»Ÿè¿è¡Œæµ‹è¯•
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"æµ‹è¯• {system_type.upper()} ç³»ç»Ÿ")
        self.logger.info(f"{'='*60}")

        for suite_name in test_suites:
            if suite_name in self.config:
                suite_results = await self._run_test_suite(system_type, suite_name)
                self.results[system_type].extend(suite_results)

        # æ‰“å°æœ€ç»ˆè¿›åº¦
        self.print_test_progress()

        # ç”Ÿæˆå•ç³»ç»ŸæŠ¥å‘Š
        return self._generate_single_system_report(system_type)

    def _generate_single_system_report(self, system_type: str) -> Dict[str, Any]:
        """ç”Ÿæˆå•ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š"""
        self.logger.info(f"ç”Ÿæˆ{system_type}ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š")

        report = {
            'timestamp': datetime.now().isoformat(),
            'system_type': system_type,
            'summary': {},
            'detailed_results': {},
            'test_categories': {},
            'recommendations': [],
            'performance_analysis': {},
            'errors': []
        }

        # åˆ†ææŒ‡å®šç³»ç»Ÿçš„ç»“æœ
        results = self.results[system_type]
        if not results:
            report['errors'].append(f"æ²¡æœ‰{system_type}ç³»ç»Ÿçš„æµ‹è¯•ç»“æœ")
            return report

        # æŒ‰æµ‹è¯•åˆ†ç±»ç»Ÿè®¡
        categories = {}
        for result in results:
            category = result.test_category
            if category not in categories:
                categories[category] = []
            categories[category].append(result)

        # è®¡ç®—ç³»ç»ŸæŒ‡æ ‡
        metrics = SystemMetrics(
            avg_response_time=sum(r.response_time for r in results) / len(results),
            avg_cpu_usage=sum(r.cpu_usage for r in results) / len(results),
            avg_memory_usage=sum(r.memory_usage for r in results) / len(results),
            total_tests=len(results),
            passed_tests=sum(1 for r in results if r.accuracy > 0.9),
            accuracy_rate=sum(r.accuracy for r in results) / len(results),
            excel_consistency=0.0,
            print_function_accuracy=sum(1 for r in results if r.print_output is not None) / len(results) if results else 0.0
        )

        report['summary'][system_type] = asdict(metrics)
        report['detailed_results'][system_type] = [asdict(r) for r in results]
        report['test_categories'][system_type] = {
            cat: {
                'total': len(cat_results),
                'avg_accuracy': sum(r.accuracy for r in cat_results) / len(cat_results),
                'avg_response_time': sum(r.response_time for r in cat_results) / len(cat_results)
            }
            for cat, cat_results in categories.items()
        }

        # é”™è¯¯ç»Ÿè®¡
        errors = [r for r in results if r.errors]
        if errors:
            report['errors'] = [f"{r.test_case}: {', '.join(r.errors)}" for r in errors]

        # ç”Ÿæˆå•ç³»ç»Ÿå»ºè®®
        report['recommendations'] = self._generate_single_system_recommendations(system_type, metrics)

        # ä¿å­˜å•ç³»ç»ŸæŠ¥å‘Š
        self._save_single_system_report(report, system_type)

        return report

    def _generate_single_system_recommendations(self, system_type: str, metrics: SystemMetrics) -> List[str]:
        """ç”Ÿæˆå•ç³»ç»Ÿæ”¹è¿›å»ºè®®"""
        recommendations = []

        if metrics.accuracy_rate < 0.9:
            recommendations.append(f"âš ï¸ {system_type}ç³»ç»Ÿå‡†ç¡®åº¦ä¸º{metrics.accuracy_rate:.1%}ï¼Œå»ºè®®ä¼˜åŒ–æ•°å­—æå–ç®—æ³•")

        if metrics.avg_response_time > 0.1:  # è¶…è¿‡100ms
            recommendations.append(f"âš ï¸ {system_type}ç³»ç»Ÿå¹³å‡å“åº”æ—¶é—´ä¸º{metrics.avg_response_time:.3f}sï¼Œå»ºè®®ä¼˜åŒ–å¤„ç†é€»è¾‘")

        if metrics.avg_cpu_usage > 50:  # CPUä½¿ç”¨ç‡è¶…è¿‡50%
            recommendations.append(f"âš ï¸ {system_type}ç³»ç»ŸCPUä½¿ç”¨ç‡ä¸º{metrics.avg_cpu_usage:.1f}%ï¼Œå»ºè®®ä¼˜åŒ–èµ„æºä½¿ç”¨")

        if metrics.avg_memory_usage > 50:  # å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡50%
            recommendations.append(f"âš ï¸ {system_type}ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡ä¸º{metrics.avg_memory_usage:.1f}%ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†")

        if not recommendations:
            recommendations.append(f"âœ… {system_type}ç³»ç»Ÿæ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œå„é¡¹æŒ‡æ ‡æ­£å¸¸")

        return recommendations

    def _save_single_system_report(self, report: Dict[str, Any], system_type: str):
        """ä¿å­˜å•ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š"""
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = Path("reports/comparison/enhanced")
        report_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜JSONæŠ¥å‘Š
        json_report_file = report_dir / f"{system_type}_system_report_{timestamp}.json"
        with open(json_report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # ç”Ÿæˆè¯¦ç»†çš„MarkdownæŠ¥å‘Š
        md_report_file = report_dir / f"{system_type}_system_report_{timestamp}.md"
        md_content = self._generate_single_system_markdown_report(report, system_type)
        with open(md_report_file, 'w', encoding='utf-8') as f:
            f.write(md_content)

        # ç”ŸæˆCSVæ•°æ®æ–‡ä»¶
        csv_data = self._generate_single_system_csv_data(report, system_type)
        csv_file = report_dir / f"{system_type}_system_data_{timestamp}.csv"
        csv_data.to_csv(csv_file, index=False, encoding='utf-8')

        self.logger.info(f"{system_type}ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
        self.logger.info(f"  JSONæŠ¥å‘Š: {json_report_file}")
        self.logger.info(f"  MarkdownæŠ¥å‘Š: {md_report_file}")
        self.logger.info(f"  CSVæ•°æ®: {csv_file}")

    def _generate_single_system_markdown_report(self, report: Dict[str, Any], system_type: str) -> str:
        """ç”Ÿæˆå•ç³»ç»ŸMarkdownæŠ¥å‘Š"""
        lines = []

        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        system_name = "åŸå§‹åŒæ­¥ç³»ç»Ÿ" if system_type == 'original' else "æ–°å¼‚æ­¥ç”Ÿäº§ç³»ç»Ÿ"
        lines.extend([
            f"# {system_name} ä¸“é¡¹æµ‹è¯•æŠ¥å‘Š",
            f"**ç”Ÿæˆæ—¶é—´**: {report['timestamp']}",
            f"**ç³»ç»Ÿç±»å‹**: {system_name}",
            "",
            "## ğŸ“Š æµ‹è¯•æ¦‚è¿°",
            "",
            f"æœ¬æ¬¡æµ‹è¯•å¯¹{system_name}è¿›è¡Œäº†å…¨é¢çš„åŠŸèƒ½æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š",
            "- âœ… æ•°å­—è¯†åˆ«å‡†ç¡®åº¦æµ‹è¯•",
            "- âœ… ç³»ç»Ÿå“åº”æ€§èƒ½æµ‹è¯•",
            "- âœ… èµ„æºä½¿ç”¨æ•ˆç‡æµ‹è¯•",
            "- âœ… PrintåŠŸèƒ½æ”¯æŒæµ‹è¯•",
            "- âœ… å„ç§å¹²æ‰°åœºæ™¯ä¸‹çš„ç¨³å®šæ€§æµ‹è¯•",
            "",
            "## ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡",
            ""
        ])

        # æ ¸å¿ƒæŒ‡æ ‡
        if system_type in report['summary']:
            metrics = report['summary'][system_type]
            lines.extend([
                "| æŒ‡æ ‡ | æ•°å€¼ | çŠ¶æ€ |",
                "|------|------|------|"
            ])

            # å‡†ç¡®åº¦
            accuracy_status = "âœ… è‰¯å¥½" if metrics['accuracy_rate'] >= 0.9 else "âš ï¸ éœ€è¦æ”¹è¿›"
            lines.append(f"| å¹³å‡å‡†ç¡®åº¦ | {metrics['accuracy_rate']:.3f} | {accuracy_status} |")

            # å“åº”æ—¶é—´
            time_status = "âœ… ä¼˜ç§€" if metrics['avg_response_time'] < 0.01 else "âœ… è‰¯å¥½" if metrics['avg_response_time'] < 0.1 else "âš ï¸ éœ€è¦ä¼˜åŒ–"
            lines.append(f"| å¹³å‡å“åº”æ—¶é—´ | {metrics['avg_response_time']:.3f}s | {time_status} |")

            # CPUä½¿ç”¨ç‡
            cpu_status = "âœ… æ­£å¸¸" if metrics['avg_cpu_usage'] < 30 else "âš ï¸ åé«˜" if metrics['avg_cpu_usage'] < 60 else "âŒ è¿‡é«˜"
            lines.append(f"| å¹³å‡CPUä½¿ç”¨ç‡ | {metrics['avg_cpu_usage']:.1f}% | {cpu_status} |")

            # å†…å­˜ä½¿ç”¨ç‡
            memory_status = "âœ… æ­£å¸¸" if metrics['avg_memory_usage'] < 50 else "âš ï¸ åé«˜" if metrics['avg_memory_usage'] < 80 else "âŒ è¿‡é«˜"
            lines.append(f"| å¹³å‡å†…å­˜ä½¿ç”¨ç‡ | {metrics['avg_memory_usage']:.1f}% | {memory_status} |")

            # é€šè¿‡ç‡
            pass_rate = metrics['passed_tests'] / metrics['total_tests'] * 100
            pass_status = "âœ… ä¼˜ç§€" if pass_rate >= 95 else "âœ… è‰¯å¥½" if pass_rate >= 85 else "âš ï¸ éœ€è¦æ”¹è¿›"
            lines.append(f"| æµ‹è¯•é€šè¿‡ç‡ | {pass_rate:.1f}% | {pass_status} |")

        lines.extend(["", "## ğŸ¯ åˆ†ç±»æµ‹è¯•ç»“æœ", ""])

        # åˆ†ç±»æµ‹è¯•ç»“æœ
        if system_type in report['test_categories']:
            lines.extend([
                "| æµ‹è¯•åˆ†ç±» | æµ‹è¯•æ•°é‡ | å¹³å‡å‡†ç¡®åº¦ | å¹³å‡å“åº”æ—¶é—´ |",
                "|----------|----------|------------|--------------|"
            ])

            for category, stats in report['test_categories'][system_type].items():
                lines.append(f"| {category} | {stats['total']} | {stats['avg_accuracy']:.3f} | {stats['avg_response_time']:.3f}s |")

        # é”™è¯¯ä¿¡æ¯
        if 'errors' in report and report['errors']:
            lines.extend(["", "## âš ï¸ é”™è¯¯ä¿¡æ¯", ""])
            for error in report['errors']:
                lines.append(f"- {error}")

        # æ”¹è¿›å»ºè®®
        if 'recommendations' in report and report['recommendations']:
            lines.extend(["", "## ğŸ’¡ æ”¹è¿›å»ºè®®", ""])
            for i, rec in enumerate(report['recommendations'], 1):
                lines.append(f"{i}. {rec}")

        lines.extend([
            "",
            "---",
            f"*æŠ¥å‘Šç”±å¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•æ¡†æ¶ç”Ÿæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        ])

        return '\n'.join(lines)

    def _generate_single_system_csv_data(self, report: Dict[str, Any], system_type: str) -> pd.DataFrame:
        """ç”Ÿæˆå•ç³»ç»ŸCSVæ•°æ®"""
        data = []

        if system_type in report['detailed_results']:
            for result in report['detailed_results'][system_type]:
                data.append({
                    'system_type': system_type,
                    'test_case': result['test_case'],
                    'input_text': result['input_text'],
                    'expected_values': str(result['expected_values']),
                    'actual_values': str(result['actual_values']),
                    'accuracy': result['accuracy'],
                    'response_time': result['response_time'],
                    'cpu_usage': result['cpu_usage'],
                    'memory_usage': result['memory_usage'],
                    'test_category': result['test_category'],
                    'has_errors': len(result['errors']) > 0
                })

        return pd.DataFrame(data)

    def print_enhanced_summary(self):
        """æ‰“å°å¢å¼ºç‰ˆæµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*70)
        print("ğŸš€ å¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•å®Œæˆæ‘˜è¦")
        print("="*70)

        for system_type, results in self.results.items():
            if not results:
                continue

            print(f"\nğŸ“Š {system_type.upper()} ç³»ç»Ÿ:")
            print(f"  ğŸ“‹ æ€»æµ‹è¯•æ•°: {len(results)}")
            print(f"  ğŸ¯ å¹³å‡å‡†ç¡®åº¦: {sum(r.accuracy for r in results)/len(results):.3f}")
            print(f"  â±ï¸  å¹³å‡å“åº”æ—¶é—´: {sum(r.response_time for r in results)/len(results):.3f}s")
            print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨: {sum(r.cpu_usage for r in results)/len(results):.1f}%")
            print(f"  ğŸ§  å¹³å‡å†…å­˜ä½¿ç”¨: {sum(r.memory_usage for r in results)/len(results):.1f}%")

            # PrintåŠŸèƒ½ç»Ÿè®¡
            print_results = [r for r in results if r.print_output is not None]
            if print_results:
                print(f"  ğŸ–¨ï¸  PrintåŠŸèƒ½æµ‹è¯•: {len(print_results)} ä¸ª")

            # é”™è¯¯ç»Ÿè®¡
            errors = [r for r in results if r.errors]
            if errors:
                print(f"  âš ï¸  é”™è¯¯æ•°: {len(errors)}")

        print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹ reports/comparison/enhanced/ ç›®å½•")

import argparse

async def main():
    """ä¸»å‡½æ•°"""
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆè¯­éŸ³è¾“å…¥ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•')
    parser.add_argument('--system', choices=['original', 'new', 'both'], default='both',
                        help='é€‰æ‹©è¦æµ‹è¯•çš„ç³»ç»Ÿ: original(åŸå§‹ç³»ç»Ÿ), new(æ–°ç³»ç»Ÿ), both(å¯¹æ¯”æµ‹è¯•)')
    args = parser.parse_args()

    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = "tests/comparison/test_config_expanded.yaml"

    # åˆ›å»ºå¢å¼ºç‰ˆæµ‹è¯•æ¡†æ¶
    framework = EnhancedComparisonFramework(config_path)

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    framework.logger.info(f"æµ‹è¯•ä¼šè¯å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    if args.system == 'both':
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæ·±åº¦å¯¹æ¯”æµ‹è¯•")
        print("å¯¹æ¯”: main.py (åŸå§‹åŒæ­¥ç³»ç»Ÿ) vs main_production.py (æ–°å¼‚æ­¥ç”Ÿäº§ç³»ç»Ÿ)")
        print("-" * 70)

        # è¿è¡Œå¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•
        report = await framework.run_enhanced_comparison()

        # è®¡ç®—æ€»è€—æ—¶
        end_time = time.time()
        total_time = end_time - start_time

        # æ‰“å°å¢å¼ºç‰ˆæ‘˜è¦
        framework.print_enhanced_summary()

        print(f"\nâœ… å¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
        print(f"â±ï¸  æ€»æµ‹è¯•è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        print(f"ğŸ“Š æ€§èƒ½åˆ†æå¾—åˆ†: {report.get('performance_analysis', {}).get('overall_score', 0):.1f}%")
        print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ reports/comparison/enhanced/ ç›®å½•")

        framework.logger.info(f"å¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•å®Œæˆ - æ€»è€—æ—¶: {total_time:.2f}ç§’, æ€§èƒ½å¾—åˆ†: {report.get('performance_analysis', {}).get('overall_score', 0):.1f}%")

    elif args.system == 'original':
        print("ğŸš€ å¼€å§‹åŸå§‹ç³»ç»Ÿä¸“é¡¹æµ‹è¯•")
        print("æµ‹è¯•: main.py (åŸå§‹åŒæ­¥ç³»ç»Ÿ)")
        print("-" * 70)

        # åªæµ‹è¯•åŸå§‹ç³»ç»Ÿ
        await framework.run_enhanced_comparison_for_system('original')
        framework.print_enhanced_summary()

        # è®¡ç®—æ€»è€—æ—¶
        end_time = time.time()
        total_time = end_time - start_time

        print(f"\nâœ… åŸå§‹ç³»ç»Ÿä¸“é¡¹æµ‹è¯•å®Œæˆï¼")
        print(f"â±ï¸  æ€»æµ‹è¯•è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")

        framework.logger.info(f"åŸå§‹ç³»ç»Ÿä¸“é¡¹æµ‹è¯•å®Œæˆ - æ€»è€—æ—¶: {total_time:.2f}ç§’")

    elif args.system == 'new':
        print("ğŸš€ å¼€å§‹æ–°ç³»ç»Ÿä¸“é¡¹æµ‹è¯•")
        print("æµ‹è¯•: main_production.py (æ–°å¼‚æ­¥ç”Ÿäº§ç³»ç»Ÿ)")
        print("-" * 70)

        # åªæµ‹è¯•æ–°ç³»ç»Ÿ
        await framework.run_enhanced_comparison_for_system('new')
        framework.print_enhanced_summary()

        # è®¡ç®—æ€»è€—æ—¶
        end_time = time.time()
        total_time = end_time - start_time

        print(f"\nâœ… æ–°ç³»ç»Ÿä¸“é¡¹æµ‹è¯•å®Œæˆï¼")
        print(f"â±ï¸  æ€»æµ‹è¯•è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")

        framework.logger.info(f"æ–°ç³»ç»Ÿä¸“é¡¹æµ‹è¯•å®Œæˆ - æ€»è€—æ—¶: {total_time:.2f}ç§’")

if __name__ == "__main__":
    asyncio.run(main())