# -*- coding: utf-8 -*-
"""
æ·±åº¦å¯¹æ¯”æµ‹è¯•æ¡†æ¶

ç”¨äºå¯¹æ¯”åŸå§‹ç³»ç»Ÿå’Œæ–°çš„å¼‚æ­¥ç”Ÿäº§ç³»ç»Ÿçš„ï¼š
- æ•°å­—è¯†åˆ«å‡†ç¡®åº¦
- æ€§èƒ½æŒ‡æ ‡ï¼ˆå“åº”æ—¶é—´ã€èµ„æºä½¿ç”¨ï¼‰
- Excelè¾“å‡ºå·®å¼‚
"""

import asyncio
import time
import sys
import os
import logging
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import pandas as pd
import psutil
import threading

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from async_config import AsyncConfigLoader


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    system_name: str
    test_timestamp: str
    accuracy_metrics: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    excel_output: Dict[str, Any] = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)


@dataclass
class AccuracyTestItem:
    """å‡†ç¡®åº¦æµ‹è¯•é¡¹ç›®"""
    input_text: str
    expected_numbers: List[float]
    test_id: str
    category: str = "general"


class ComparisonTestFramework:
    """å¯¹æ¯”æµ‹è¯•æ¡†æ¶"""

    def __init__(self, test_config_path: str):
        self.test_config_path = test_config_path
        self.config_loader = AsyncConfigLoader(test_config_path, enable_hot_reload=False)
        self.logger = self._setup_logger()

        # æµ‹è¯•æ•°æ®
        self.test_data = [
            AccuracyTestItem("æ¸©åº¦äºŒåäº”ç‚¹äº”åº¦", [25.5], "TEMP001", "temperature"),
            AccuracyTestItem("å‹åŠ›ä¸€ç™¾äºŒåå¸•æ–¯å¡", [120.0], "PRES001", "pressure"),
            AccuracyTestItem("æ¹¿åº¦ç™¾åˆ†ä¹‹ä¸ƒåäº”", [75.0], "HUMI001", "humidity"),
            AccuracyTestItem("é•¿åº¦ä¸ºä¸€ç±³äºŒ", [1.2], "LENG001", "length"),
            AccuracyTestItem("æ•°å€¼42", [42.0], "NUM001", "direct"),
            AccuracyTestItem("è§’åº¦ä¹ååº¦", [90.0], "ANG001", "angle"),
            AccuracyTestItem("é‡é‡åäº”åƒå…‹", [15.0], "WEIG001", "weight"),
            AccuracyTestItem("æ·±åº¦è´Ÿåç‚¹äº”ç±³", [-10.5], "DEPT001", "depth"),
            AccuracyTestItem("è®¡æ•°ä¸€åƒé›¶ä¸€", [1001.0], "COUNT001", "count"),
            AccuracyTestItem("é€Ÿåº¦å…«åå…«å…¬é‡Œæ¯å°æ—¶", [88.0], "SPEED001", "speed"),
            # æ›´å¤æ‚çš„æµ‹è¯•æ¡ˆä¾‹
            AccuracyTestItem("æ¸©åº¦ä¸‰åä¸ƒç‚¹äº”åº¦å¿ƒç‡ä¸ƒåäº”", [37.5, 75.0], "MULT001", "multiple"),
            AccuracyTestItem("é›¶åº¦", [0.0], "ZERO001", "edge_case"),
            AccuracyTestItem("è´Ÿäº”åº¦", [-5.0], "NEG001", "negative"),
            AccuracyTestItem("ä¸€ç™¾ç‚¹é›¶ä¸€åº¦", [100.01], "DEC001", "decimal"),
        ]

        # æµ‹è¯•ç»“æœ
        self.results: Dict[str, TestResult] = {}

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('comparison_test')
        logger.setLevel(logging.INFO)

        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        log_dir = Path("logs/comparison")
        log_dir.mkdir(parents=True, exist_ok=True)

        handler = logging.FileHandler(log_dir / f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)

        return logger

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶"""
        try:
            success = await self.config_loader.initialize()
            if not success:
                self.logger.error("æµ‹è¯•é…ç½®åŠ è½½å¤±è´¥")
                return False

            self.logger.info("å¯¹æ¯”æµ‹è¯•æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
            return True

        except Exception as e:
            self.logger.error(f"æµ‹è¯•æ¡†æ¶åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def run_accuracy_test(self, system_name: str, test_wrapper) -> Dict[str, Any]:
        """è¿è¡Œå‡†ç¡®åº¦æµ‹è¯•"""
        self.logger.info(f"å¼€å§‹ {system_name} å‡†ç¡®åº¦æµ‹è¯•")

        correct_count = 0
        total_count = len(self.test_data)
        detailed_results = []
        category_stats = {}

        for item in self.test_data:
            try:
                # æ¨¡æ‹Ÿè¯­éŸ³è¾“å…¥å¤„ç†
                start_time = time.time()
                extracted_numbers = await test_wrapper.process_text(item.input_text)
                processing_time = time.time() - start_time

                # æ£€æŸ¥å‡†ç¡®æ€§
                is_correct = self._compare_numbers(extracted_numbers, item.expected_numbers)
                if is_correct:
                    correct_count += 1

                # è®°å½•è¯¦ç»†ç»“æœ
                result = {
                    'test_id': item.test_id,
                    'input_text': item.input_text,
                    'expected': item.expected_numbers,
                    'actual': extracted_numbers,
                    'correct': is_correct,
                    'processing_time': processing_time,
                    'category': item.category
                }
                detailed_results.append(result)

                # åˆ†ç±»ç»Ÿè®¡
                if item.category not in category_stats:
                    category_stats[item.category] = {'correct': 0, 'total': 0}
                category_stats[item.category]['total'] += 1
                if is_correct:
                    category_stats[item.category]['correct'] += 1

                self.logger.info(f"{system_name} æµ‹è¯• {item.test_id}: {item.input_text} -> {extracted_numbers} ({'âœ“' if is_correct else 'âœ—'})")

            except Exception as e:
                self.logger.error(f"{system_name} æµ‹è¯• {item.test_id} å¤±è´¥: {e}")
                detailed_results.append({
                    'test_id': item.test_id,
                    'input_text': item.input_text,
                    'expected': item.expected_numbers,
                    'actual': [],
                    'correct': False,
                    'error': str(e),
                    'category': item.category
                })

        # è®¡ç®—å‡†ç¡®åº¦æŒ‡æ ‡
        overall_accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0

        category_accuracies = {}
        for category, stats in category_stats.items():
            accuracy = (stats['correct'] / stats['total']) * 100 if stats['total'] > 0 else 0
            category_accuracies[category] = {
                'accuracy': accuracy,
                'correct': stats['correct'],
                'total': stats['total']
            }

        return {
            'overall_accuracy': overall_accuracy,
            'category_accuracies': category_accuracies,
            'detailed_results': detailed_results,
            'total_tests': total_count,
            'correct_tests': correct_count
        }

    async def run_performance_test(self, system_name: str, test_wrapper) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        self.logger.info(f"å¼€å§‹ {system_name} æ€§èƒ½æµ‹è¯•")

        # ç³»ç»Ÿèµ„æºä½¿ç”¨ç›‘æ§
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        initial_cpu = process.cpu_percent()

        # å“åº”æ—¶é—´æµ‹è¯•
        response_times = []
        throughput_times = []

        # å¯åŠ¨ç³»ç»Ÿ
        startup_start = time.time()
        await test_wrapper.startup()
        startup_time = time.time() - startup_start

        # å¹¶å‘å¤„ç†æµ‹è¯•
        concurrent_start = time.time()
        tasks = []
        test_texts = [item.input_text for item in self.test_data[:5]]  # ä½¿ç”¨å‰5ä¸ªæµ‹è¯•æ¡ˆä¾‹

        for text in test_texts:
            task = asyncio.create_task(test_wrapper.process_text(text))
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        concurrent_time = time.time() - concurrent_start

        # è®°å½•æˆåŠŸçš„å“åº”æ—¶é—´
        successful_results = [r for r in results if not isinstance(r, Exception)]
        if successful_results:
            response_times = [time.time() for _ in successful_results]  # ç®€åŒ–çš„å“åº”æ—¶é—´è®°å½•

        # èµ„æºä½¿ç”¨æµ‹é‡
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        final_cpu = process.cpu_percent()

        # å…³é—­ç³»ç»Ÿ
        shutdown_start = time.time()
        await test_wrapper.shutdown()
        shutdown_time = time.time() - shutdown_start

        return {
            'startup_time': startup_time,
            'shutdown_time': shutdown_time,
            'concurrent_processing_time': concurrent_time,
            'memory_usage_mb': final_memory - initial_memory,
            'cpu_usage_percent': final_cpu - initial_cpu,
            'throughput': len(successful_results) / concurrent_time if concurrent_time > 0 else 0,
            'successful_operations': len(successful_results),
            'failed_operations': len(results) - len(successful_results)
        }

    async def run_excel_comparison(self, system_name: str, test_wrapper) -> Dict[str, Any]:
        """è¿è¡ŒExcelè¾“å‡ºå¯¹æ¯”æµ‹è¯•"""
        self.logger.info(f"å¼€å§‹ {system_name} Excelè¾“å‡ºæµ‹è¯•")

        try:
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = [
                {"ID": "TEST001", "æ•°å€¼": 25.5, "åŸå§‹æ–‡æœ¬": "æ¸©åº¦äºŒåäº”ç‚¹äº”åº¦"},
                {"ID": "TEST002", "æ•°å€¼": 120.0, "åŸå§‹æ–‡æœ¬": "å‹åŠ›ä¸€ç™¾äºŒåå¸•æ–¯å¡"},
                {"ID": "TEST003", "æ•°å€¼": [37.5, 75.0], "åŸå§‹æ–‡æœ¬": "æ¸©åº¦ä¸‰åä¸ƒç‚¹äº”åº¦å¿ƒç‡ä¸ƒåäº”"},
            ]

            # å¤„ç†æ•°æ®å¹¶ç”ŸæˆExcel
            start_time = time.time()
            excel_path = await test_wrapper.generate_excel(test_data, f"test_{system_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")
            generation_time = time.time() - start_time

            if not excel_path or not Path(excel_path).exists():
                return {
                    'success': False,
                    'error': 'Excelæ–‡ä»¶ç”Ÿæˆå¤±è´¥',
                    'generation_time': generation_time
                }

            # åˆ†æExcelæ–‡ä»¶
            excel_analysis = self._analyze_excel_file(excel_path)

            return {
                'success': True,
                'file_path': excel_path,
                'generation_time': generation_time,
                'file_size_kb': Path(excel_path).stat().st_size / 1024,
                'analysis': excel_analysis
            }

        except Exception as e:
            self.logger.error(f"{system_name} Excelæµ‹è¯•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'generation_time': 0
            }

    def _analyze_excel_file(self, excel_path: str) -> Dict[str, Any]:
        """åˆ†æExcelæ–‡ä»¶"""
        try:
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(excel_path)

            analysis = {
                'row_count': len(df),
                'column_count': len(df.columns),
                'columns': list(df.columns),
                'data_types': df.dtypes.to_dict(),
                'sample_data': df.head().to_dict('records') if len(df) > 0 else []
            }

            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_columns = ['ID', 'æ•°å€¼', 'åŸå§‹æ–‡æœ¬']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                analysis['warnings'] = [f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}"]

            return analysis

        except Exception as e:
            return {
                'error': f"Excelæ–‡ä»¶åˆ†æå¤±è´¥: {e}",
                'row_count': 0,
                'column_count': 0
            }

    def _compare_numbers(self, extracted: List[float], expected: List[float]) -> bool:
        """æ¯”è¾ƒæ•°å­—æ˜¯å¦åŒ¹é…"""
        if len(extracted) != len(expected):
            return False

        tolerance = 0.01  # å…è®¸çš„è¯¯å·®èŒƒå›´
        for ext, exp in zip(extracted, expected):
            if abs(ext - exp) > tolerance:
                return False

        return True

    async def run_full_comparison(self) -> Dict[str, TestResult]:
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”æµ‹è¯•"""
        self.logger.info("å¼€å§‹å®Œæ•´å¯¹æ¯”æµ‹è¯•")

        # æµ‹è¯•ç³»ç»ŸåŒ…è£…å™¨
        original_wrapper = OriginalSystemWrapper()
        production_wrapper = ProductionSystemWrapper()

        systems = [
            ("OriginalSystem", original_wrapper),
            ("ProductionSystem", production_wrapper)
        ]

        for system_name, wrapper in systems:
            self.logger.info(f"æµ‹è¯•ç³»ç»Ÿ: {system_name}")

            result = TestResult(
                system_name=system_name,
                test_timestamp=datetime.now().isoformat()
            )

            try:
                # å‡†ç¡®åº¦æµ‹è¯•
                result.accuracy_metrics = await self.run_accuracy_test(system_name, wrapper)

                # æ€§èƒ½æµ‹è¯•
                result.performance_metrics = await self.run_performance_test(system_name, wrapper)

                # Excelè¾“å‡ºæµ‹è¯•
                result.excel_output = await self.run_excel_comparison(system_name, wrapper)

            except Exception as e:
                result.errors.append(f"ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
                self.logger.error(f"{system_name} æµ‹è¯•å¤±è´¥: {e}")

            self.results[system_name] = result

        return self.results

    def generate_comparison_report(self) -> str:
        """ç”Ÿæˆå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š"""
        report_path = f"tests/comparison/comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        report_content = [
            "# ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## æµ‹è¯•æ¦‚è¿°",
            "",
            f"æµ‹è¯•ç³»ç»Ÿæ•°é‡: {len(self.results)}",
            f"æµ‹è¯•æ¡ˆä¾‹æ•°é‡: {len(self.test_data)}",
            "",
            "## æµ‹è¯•ç»“æœå¯¹æ¯”",
            ""
        ]

        # å‡†ç¡®åº¦å¯¹æ¯”
        report_content.extend([
            "### 1. æ•°å­—è¯†åˆ«å‡†ç¡®åº¦å¯¹æ¯”",
            "",
            "| ç³»ç»Ÿ | æ€»ä½“å‡†ç¡®åº¦ | æ­£ç¡®/æ€»æ•° | è¯¦ç»†ä¿¡æ¯ |",
            "|------|------------|-----------|----------|"
        ])

        for system_name, result in self.results.items():
            accuracy = result.accuracy_metrics.get('overall_accuracy', 0)
            correct = result.accuracy_metrics.get('correct_tests', 0)
            total = result.accuracy_metrics.get('total_tests', 0)
            report_content.append(f"| {system_name} | {accuracy:.1f}% | {correct}/{total} | [æŸ¥çœ‹è¯¦æƒ…](#accuracy-{system_name.lower()}) |")

        # æ€§èƒ½å¯¹æ¯”
        report_content.extend([
            "",
            "### 2. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”",
            "",
            "| ç³»ç»Ÿ | å¯åŠ¨æ—¶é—´(s) | å¹¶å‘å¤„ç†æ—¶é—´(s) | å†…å­˜ä½¿ç”¨(MB) | ååé‡(ops/s) |",
            "|------|-------------|------------------|---------------|----------------|"
        ])

        for system_name, result in self.results.items():
            startup = result.performance_metrics.get('startup_time', 0)
            concurrent = result.performance_metrics.get('concurrent_processing_time', 0)
            memory = result.performance_metrics.get('memory_usage_mb', 0)
            throughput = result.performance_metrics.get('throughput', 0)
            report_content.append(f"| {system_name} | {startup:.3f} | {concurrent:.3f} | {memory:.1f} | {throughput:.1f} |")

        # Excelè¾“å‡ºå¯¹æ¯”
        report_content.extend([
            "",
            "### 3. Excelè¾“å‡ºå¯¹æ¯”",
            "",
            "| ç³»ç»Ÿ | ç”ŸæˆæˆåŠŸ | æ–‡ä»¶å¤§å°(KB) | ç”Ÿæˆæ—¶é—´(s) | è¡Œæ•° | åˆ—æ•° |",
            "|------|----------|--------------|-------------|------|------|"
        ])

        for system_name, result in self.results.items():
            success = result.excel_output.get('success', False)
            size = result.excel_output.get('file_size_kb', 0)
            time_taken = result.excel_output.get('generation_time', 0)
            rows = result.excel_output.get('analysis', {}).get('row_count', 0)
            cols = result.excel_output.get('analysis', {}).get('column_count', 0)
            report_content.append(f"| {system_name} | {'âœ“' if success else 'âœ—'} | {size:.1f} | {time_taken:.3f} | {rows} | {cols} |")

        # è¯¦ç»†ç»“æœ
        for system_name, result in self.results.items():
            report_content.extend([
                "",
                f"## è¯¦ç»†ç»“æœ - {system_name}",
                f"<a id=\"accuracy-{system_name.lower()}\"></a>",
                "",
                "### å‡†ç¡®åº¦è¯¦æƒ…",
                ""
            ])

            # åˆ†ç±»å‡†ç¡®åº¦
            category_accuracies = result.accuracy_metrics.get('category_accuracies', {})
            for category, stats in category_accuracies.items():
                report_content.extend([
                    f"**{category}**: {stats['accuracy']:.1f}% ({stats['correct']}/{stats['total']})",
                    ""
                ])

            # é”™è¯¯åˆ†æ
            detailed_results = result.accuracy_metrics.get('detailed_results', [])
            error_results = [r for r in detailed_results if not r.get('correct', False)]

            if error_results:
                report_content.extend([
                    "### è¯†åˆ«é”™è¯¯æ¡ˆä¾‹",
                    "",
                    "| æµ‹è¯•ID | è¾“å…¥æ–‡æœ¬ | æœŸæœ›ç»“æœ | å®é™…ç»“æœ |",
                    "|--------|----------|----------|----------|"
                ])

                for error in error_results[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
                    test_id = error.get('test_id', 'N/A')
                    input_text = error.get('input_text', 'N/A')
                    expected = str(error.get('expected', []))
                    actual = str(error.get('actual', []))
                    report_content.append(f"| {test_id} | {input_text} | {expected} | {actual} |")

        # å·®å¼‚åˆ†æå’Œå»ºè®®
        report_content.extend([
            "",
            "## å·®å¼‚åˆ†æå’Œæ”¹è¿›å»ºè®®",
            "",
            "### ä¸»è¦å·®å¼‚",
            "",
        ])

        # æ·»åŠ å…·ä½“çš„å·®å¼‚åˆ†æ
        if len(self.results) >= 2:
            original_result = self.results.get("OriginalSystem")
            production_result = self.results.get("ProductionSystem")

            if original_result and production_result:
                # å‡†ç¡®åº¦å·®å¼‚
                orig_accuracy = original_result.accuracy_metrics.get('overall_accuracy', 0)
                prod_accuracy = production_result.accuracy_metrics.get('overall_accuracy', 0)
                accuracy_diff = prod_accuracy - orig_accuracy

                report_content.extend([
                    f"- **å‡†ç¡®åº¦å·®å¼‚**: ç”Ÿäº§ç³»ç»Ÿç›¸æ¯”åŸå§‹ç³»ç»Ÿ {'æå‡' if accuracy_diff > 0 else 'ä¸‹é™'} {abs(accuracy_diff):.1f}%",
                    ""
                ])

                # æ€§èƒ½å·®å¼‚
                orig_throughput = original_result.performance_metrics.get('throughput', 0)
                prod_throughput = production_result.performance_metrics.get('throughput', 0)
                throughput_change = ((prod_throughput - orig_throughput) / orig_throughput * 100) if orig_throughput > 0 else 0

                report_content.extend([
                    f"- **ååé‡å˜åŒ–**: ç”Ÿäº§ç³»ç»Ÿç›¸æ¯”åŸå§‹ç³»ç»Ÿ {'æå‡' if throughput_change > 0 else 'ä¸‹é™'} {abs(throughput_change):.1f}%",
                    ""
                ])

        report_content.extend([
            "### æ”¹è¿›å»ºè®®",
            "",
            "1. **ä¸­æ–‡æ•°å­—è¯†åˆ«ä¼˜åŒ–**: åŸºäºé”™è¯¯æ¡ˆä¾‹ï¼Œä¼˜åŒ–ä¸­æ–‡æ•°å­—åˆ°æ•°å­—çš„è½¬æ¢ç®—æ³•",
            "2. **æ€§èƒ½ä¼˜åŒ–**: é’ˆå¯¹è¯†åˆ«å‡ºçš„æ€§èƒ½ç“¶é¢ˆè¿›è¡Œä¼˜åŒ–",
            "3. **Excelæ ¼å¼æ ‡å‡†åŒ–**: ç¡®ä¿ä¸¤ä¸ªç³»ç»Ÿç”Ÿæˆç›¸åŒæ ¼å¼çš„Excelè¾“å‡º",
            "4. **é”™è¯¯å¤„ç†å¢å¼º**: æ”¹è¿›å¼‚å¸¸æƒ…å†µçš„å¤„ç†å’Œæ¢å¤æœºåˆ¶",
            "",
            "---",
            f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        ])

        # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
        Path(report_path).parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))

        self.logger.info(f"å¯¹æ¯”æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path


class OriginalSystemWrapper:
    """åŸå§‹ç³»ç»Ÿæµ‹è¯•åŒ…è£…å™¨"""

    def __init__(self):
        self.process = None
        self.logger = logging.getLogger('original_wrapper')

    async def startup(self):
        """å¯åŠ¨åŸå§‹ç³»ç»Ÿ"""
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é€‚é…åŸå§‹ç³»ç»Ÿçš„å¯åŠ¨æ–¹å¼
        # ç”±äºåŸå§‹ç³»ç»Ÿå¯èƒ½ä¸æ˜¯å¼‚æ­¥çš„ï¼Œéœ€è¦åœ¨çº¿ç¨‹ä¸­è¿è¡Œ
        pass

    async def shutdown(self):
        """å…³é—­åŸå§‹ç³»ç»Ÿ"""
        pass

    async def process_text(self, text: str) -> List[float]:
        """å¤„ç†æ–‡æœ¬ï¼ˆæ¨¡æ‹ŸåŸå§‹ç³»ç»Ÿçš„å¤„ç†é€»è¾‘ï¼‰"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨åŸå§‹ç³»ç»Ÿçš„æ–‡æœ¬å¤„ç†é€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
        from audio_capture_v import extract_measurements
        return extract_measurements(text)

    async def generate_excel(self, data: List[Dict], filename: str) -> str:
        """ç”ŸæˆExcelæ–‡ä»¶ï¼ˆä½¿ç”¨åŸå§‹ç³»ç»Ÿçš„Excelå¯¼å‡ºå™¨ï¼‰"""
        from excel_exporter import ExcelExporter

        output_dir = Path("tests/comparison/output")
        output_dir.mkdir(parents=True, exist_ok=True)

        exporter = ExcelExporter()
        output_path = output_dir / filename

        # é€‚é…æ•°æ®æ ¼å¼
        for item in data:
            if isinstance(item.get('æ•°å€¼'), list):
                item['æ•°å€¼'] = ', '.join(map(str, item['æ•°å€¼']))

        await exporter.export_data(data, str(output_path))
        return str(output_path)


class ProductionSystemWrapper:
    """ç”Ÿäº§ç³»ç»Ÿæµ‹è¯•åŒ…è£…å™¨"""

    def __init__(self):
        self.system = None
        self.logger = logging.getLogger('production_wrapper')

    async def startup(self):
        """å¯åŠ¨ç”Ÿäº§ç³»ç»Ÿ"""
        # åŠ¨æ€å¯¼å…¥ç”Ÿäº§ç³»ç»Ÿ
        try:
            from main_production import ProductionVoiceSystem
            config_path = "tests/comparison/test_config.yaml"
            self.system = ProductionVoiceSystem(config_path)
            await self.system.start()
        except Exception as e:
            self.logger.error(f"ç”Ÿäº§ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
            raise

    async def shutdown(self):
        """å…³é—­ç”Ÿäº§ç³»ç»Ÿ"""
        if self.system:
            await self.system.stop()

    async def process_text(self, text: str) -> List[float]:
        """å¤„ç†æ–‡æœ¬ï¼ˆä½¿ç”¨ç”Ÿäº§ç³»ç»Ÿçš„å¤„ç†é€»è¾‘ï¼‰"""
        if not self.system:
            # å¦‚æœç³»ç»Ÿæœªå¯åŠ¨ï¼Œä½¿ç”¨åŸºæœ¬çš„å¤„ç†é€»è¾‘
            from audio_capture_v import extract_measurements
            return extract_measurements(text)

        # è¿™é‡Œåº”è¯¥è°ƒç”¨ç”Ÿäº§ç³»ç»Ÿçš„æ–‡æœ¬å¤„ç†é€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
        from audio_capture_v import extract_measurements
        return extract_measurements(text)

    async def generate_excel(self, data: List[Dict], filename: str) -> str:
        """ç”ŸæˆExcelæ–‡ä»¶ï¼ˆä½¿ç”¨ç”Ÿäº§ç³»ç»Ÿçš„Excelå¯¼å‡ºå™¨ï¼‰"""
        from excel_exporter import ExcelExporter

        output_dir = Path("tests/comparison/output")
        output_dir.mkdir(parents=True, exist_ok=True)

        exporter = ExcelExporter()
        output_path = output_dir / filename

        # é€‚é…æ•°æ®æ ¼å¼
        for item in data:
            if isinstance(item.get('æ•°å€¼'), list):
                item['æ•°å€¼'] = ', '.join(map(str, item['æ•°å€¼']))

        await exporter.export_data(data, str(output_path))
        return str(output_path)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ”¬ å¼€å§‹æ·±åº¦å¯¹æ¯”æµ‹è¯•")

    # åˆ›å»ºæµ‹è¯•æ¡†æ¶
    test_framework = ComparisonTestFramework("tests/comparison/test_config.yaml")

    # åˆå§‹åŒ–
    if not await test_framework.initialize():
        print("âŒ æµ‹è¯•æ¡†æ¶åˆå§‹åŒ–å¤±è´¥")
        return 1

    try:
        # è¿è¡Œå®Œæ•´å¯¹æ¯”æµ‹è¯•
        results = await test_framework.run_full_comparison()

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        report_path = test_framework.generate_comparison_report()

        print(f"âœ… å¯¹æ¯”æµ‹è¯•å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

        # è¾“å‡ºç®€è¦ç»“æœ
        print("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
        for system_name, result in results.items():
            accuracy = result.accuracy_metrics.get('overall_accuracy', 0)
            throughput = result.performance_metrics.get('throughput', 0)
            excel_success = result.excel_output.get('success', False)

            print(f"  {system_name}:")
            print(f"    å‡†ç¡®åº¦: {accuracy:.1f}%")
            print(f"    ååé‡: {throughput:.1f} ops/s")
            print(f"    Excelè¾“å‡º: {'âœ“' if excel_success else 'âœ—'}")

        return 0

    except Exception as e:
        print(f"âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)