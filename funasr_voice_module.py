#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FunASRè¯­éŸ³è¯†åˆ«æ¨¡å—
åŸºäºFunASRçš„è¯­éŸ³å½•å…¥å’Œè¯†åˆ«åŠŸèƒ½ï¼Œå¯ä½œä¸ºæ¨¡å—å¯¼å…¥ä½¿ç”¨
ç»“åˆVADã€æµå¼è¯†åˆ«å’Œå¤šç§ä¼˜åŒ–ç­–ç•¥

ä½¿ç”¨ç¤ºä¾‹:
    from funasr_voice_module import FunASRVoiceRecognizer

    recognizer = FunASRVoiceRecognizer()
    recognizer.initialize()
    result = recognizer.recognize_speech(duration=10)
    print(f"è¯†åˆ«ç»“æœ: {result}")
"""

import os
import sys
import warnings

# å½»åº•æŠ‘åˆ¶FunASRçš„è¿›åº¦æ¡å’Œè°ƒè¯•è¾“å‡º
os.environ['TQDM_DISABLE'] = '1'
os.environ['PYTHONWARNINGS'] = 'ignore'
os.environ['HIDE_PROGRESS'] = '1'
os.environ['FUNASR_LOG_LEVEL'] = 'ERROR'

# é…ç½®æ—¥å¿—çº§åˆ«ï¼Œåªæ˜¾ç¤ºé”™è¯¯
import logging
logging.getLogger("funasr").setLevel(logging.ERROR)
logging.getLogger("modelscope").setLevel(logging.ERROR)
logging.getLogger("transformers").setLevel(logging.ERROR)

warnings.filterwarnings('ignore')

# ============================================================================
# ğŸ”§ å…³é”®ï¼šFFmpegç¯å¢ƒå¿…é¡»åœ¨FunASRå¯¼å…¥å‰è®¾ç½®
# ============================================================================
def setup_ffmpeg_environment():
    """è®¾ç½®FFmpegç¯å¢ƒï¼ˆå¿…é¡»åœ¨å¯¼å…¥FunASRä¹‹å‰è°ƒç”¨ï¼‰"""
    try:
        # è·å–å½“å‰è„šæœ¬ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))

        # å°è¯•å¤šä¸ªå¯èƒ½çš„FFmpegè·¯å¾„
        ffmpeg_paths = [
            # 1. FunASR_Deploymentç›®å½•ä¸‹çš„FFmpeg
            os.path.join(script_dir, "FunASR_Deployment", "dependencies",
                        "ffmpeg-master-latest-win64-gpl-shared", "bin"),
            # 2. Fç›˜æ ¹ç›®å½•ä¸‹çš„FFmpeg
            "F:/onnx_deps/ffmpeg-master-latest-win64-gpl-shared/bin",
            # 3. ç³»ç»ŸPATHä¸­çš„FFmpegï¼ˆå¦‚æœå·²å®‰è£…ï¼‰
            ""
        ]

        ffmpeg_found = False
        for ffmpeg_path in ffmpeg_paths:
            if ffmpeg_path and os.path.exists(ffmpeg_path):
                current_path = os.environ.get('PATH', '')
                if ffmpeg_path not in current_path:
                    os.environ['PATH'] = ffmpeg_path + os.pathsep + current_path
                    print(f"ğŸ”§ è®¾ç½®FFmpegè·¯å¾„: {ffmpeg_path}")
                    ffmpeg_found = True
                    break
            elif not ffmpeg_path:  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ£€æŸ¥ç³»ç»ŸPATH
                # æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦å·²æœ‰FFmpeg
                try:
                    import subprocess
                    result = subprocess.run(['ffmpeg', '-version'],
                                          capture_output=True, text=True, timeout=3)
                    if result.returncode == 0:
                        print("âœ… ç³»ç»ŸPATHä¸­å·²å­˜åœ¨FFmpeg")
                        ffmpeg_found = True
                        break
                except:
                    pass

        if not ffmpeg_found:
            print("âš ï¸ æœªæ‰¾åˆ°FFmpegï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
            print("ğŸ’¡ å»ºè®®ï¼šå°†FFmpegå®‰è£…åˆ°ç³»ç»ŸPATHæˆ–æ”¾ç½®åœ¨FunASR_Deployment/dependencies/ç›®å½•ä¸‹")

        return ffmpeg_found

    except Exception as e:
        print(f"âš ï¸ FFmpegç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
        return False

# ç«‹å³æ‰§è¡ŒFFmpegç¯å¢ƒè®¾ç½®
setup_ffmpeg_environment()

# ============================================================================
# ğŸ“¦ å¯¼å…¥å…¶ä»–ä¾èµ–
# ============================================================================
import io
import time
import logging
import numpy as np
import pyaudio
import threading
from contextlib import contextmanager
from typing import List, Dict, Optional, Callable, Union, Tuple, Any
from dataclasses import dataclass
from collections import deque

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å…¨å±€å¯ç”¨æ€§æ£€æŸ¥
FUNASR_AVAILABLE = False
PYAUDIO_AVAILABLE = False
NUMPY_AVAILABLE = False

# æ£€æŸ¥ä¾èµ–
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    logger.error("âŒ numpy ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install numpy")
    # ä¸è®¾ç½® np å˜é‡ï¼Œé¿å…ç±»å‹å†²çª

try:
    import pyaudio
    PYAUDIO_AVAILABLE = True
except ImportError:
    logger.error("âŒ pyaudio ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install pyaudio")

try:
    from funasr import AutoModel
    FUNASR_AVAILABLE = True
    logger.info("âœ… FunASR æ¨¡å—å¯ç”¨")
except ImportError as e:
    logger.error(f"âŒ FunASR ä¸å¯ç”¨: {e}")
    AutoModel = None

@dataclass
class RecognitionResult:
    """è¯­éŸ³è¯†åˆ«ç»“æœæ•°æ®ç±»"""
    text: str                    # æœ€ç»ˆè¯†åˆ«æ–‡æœ¬
    partial_results: List[str]   # éƒ¨åˆ†è¯†åˆ«ç»“æœåˆ—è¡¨
    confidence: float            # ç½®ä¿¡åº¦
    duration: float              # è¯†åˆ«æ—¶é•¿
    timestamp: float             # æ—¶é—´æˆ³
    audio_buffer: List[np.ndarray]  # éŸ³é¢‘ç¼“å†²åŒº

@dataclass
class VADConfig:
    """VADé…ç½®"""
    energy_threshold: float = 0.015
    min_speech_duration: float = 0.3
    min_silence_duration: float = 0.6
    speech_padding: float = 0.3

@dataclass
class FunASRConfig:
    """FunASRé…ç½®"""
    model_path: str = "f:/04_AI/01_Workplace/Voice_Input/model/fun"
    device: str = "cpu"
    chunk_size: Optional[List[int]] = None
    encoder_chunk_look_back: int = 4
    decoder_chunk_look_back: int = 1
    disable_update: bool = True
    trust_remote_code: bool = False

    def __post_init__(self):
        if self.chunk_size is None:
            self.chunk_size = [0, 10, 5]  # é»˜è®¤æµå¼å‚æ•°

class FunASRVoiceRecognizer:
    """
    FunASRè¯­éŸ³è¯†åˆ«å™¨ä¸»ç±»
    æä¾›è¯­éŸ³å½•å…¥ã€è¯†åˆ«å’ŒVADåŠŸèƒ½
    """

    def __init__(self,
                 model_path: Optional[str] = None,
                 device: str = "cpu",
                 sample_rate: int = 16000,
                 chunk_size: int = 1600,
                 silent_mode: bool = True):
        """
        åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨

        Args:
            model_path: FunASRæ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ç±»å‹ ("cpu" æˆ– "cuda")
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
            chunk_size: éŸ³é¢‘å—å¤§å°
            silent_mode: é™é»˜æ¨¡å¼ï¼Œéšè—ä¸­é—´è¿‡ç¨‹ä¿¡æ¯
        """
        # åŸºç¡€é…ç½®
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.model_path = model_path or "./model/fun"
        self.silent_mode = silent_mode

        # FunASRé…ç½®
        self.funasr_config = FunASRConfig(
            model_path=self.model_path,
            device=device
        )

        # VADé…ç½®
        self.vad_config = VADConfig()

        # æ¨¡å‹ç›¸å…³
        self._model: Optional[Any] = None
        self._model_loaded = False
        self._model_load_time = 0.0

        # è¿è¡ŒçŠ¶æ€
        self._is_initialized = False
        self._is_running = False
        self._stop_event = threading.Event()
        self._speech_detected = False

        # éŸ³é¢‘å¤„ç†
        self._audio_buffer: deque[np.ndarray] = deque(maxlen=sample_rate * 5)  # 5ç§’ç¼“å†²
        self._speech_buffer: List[np.ndarray] = []
        self._funasr_cache: Dict[str, Any] = {}

        # è¯†åˆ«ç»“æœ
        self._current_text = ""
        self._partial_results: List[str] = []
        self._final_results: List[RecognitionResult] = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_recognitions': 0,
            'successful_recognitions': 0,
            'total_audio_time': 0.0,
            'total_processing_time': 0.0,
            'average_confidence': 0.0
        }

        # å›è°ƒå‡½æ•°
        self._on_partial_result: Optional[Callable[[str], None]] = None
        self._on_final_result: Optional[Callable[[RecognitionResult], None]] = None
        self._on_vad_event: Optional[Callable[[str, Dict], None]] = None

    def set_callbacks(self,
                     on_partial_result: Optional[Callable[[str], None]] = None,
                     on_final_result: Optional[Callable[[RecognitionResult], None]] = None,
                     on_vad_event: Optional[Callable[[str, Dict], None]] = None):
        """
        è®¾ç½®å›è°ƒå‡½æ•°

        Args:
            on_partial_result: éƒ¨åˆ†ç»“æœå›è°ƒ
            on_final_result: æœ€ç»ˆç»“æœå›è°ƒ
            on_vad_event: VADäº‹ä»¶å›è°ƒ
        """
        self._on_partial_result = on_partial_result
        self._on_final_result = on_final_result
        self._on_vad_event = on_vad_event

    def setup_environment(self) -> bool:
        """è®¾ç½®è¿è¡Œç¯å¢ƒ"""
        try:
            # è®¾ç½®FFmpegè·¯å¾„ï¼ˆå¦‚æœéœ€è¦ï¼‰
            script_dir = os.path.dirname(os.path.abspath(__file__))
            local_ffmpeg_bin = os.path.join(script_dir, "FunASR_Deployment",
                                          "dependencies", "ffmpeg-master-latest-win64-gpl-shared", "bin")

            if os.path.exists(local_ffmpeg_bin):
                current_path = os.environ.get('PATH', '')
                if local_ffmpeg_bin not in current_path:
                    os.environ['PATH'] = local_ffmpeg_bin + os.pathsep + current_path
                    logger.info(f"ğŸ”§ è®¾ç½®æœ¬åœ°FFmpeg: {local_ffmpeg_bin}")

            return True

        except Exception as e:
            logger.warning(f"ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
            return False

    def check_dependencies(self) -> bool:
        """æ£€æŸ¥ä¾èµ–æ˜¯å¦æ»¡è¶³"""
        missing_deps = []

        if not NUMPY_AVAILABLE:
            missing_deps.append("numpy")
        if not PYAUDIO_AVAILABLE:
            missing_deps.append("pyaudio")
        if not FUNASR_AVAILABLE:
            missing_deps.append("funasr")

        if missing_deps:
            logger.error(f"âŒ ç¼ºå°‘ä¾èµ–: {', '.join(missing_deps)}")
            logger.error("è¯·æ‰§è¡Œ: pip install " + " ".join(missing_deps))
            return False

        return True

    def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        if self._is_initialized:
            logger.info("âœ… è¯†åˆ«å™¨å·²åˆå§‹åŒ–")
            return True

        logger.info("ğŸš€ åˆå§‹åŒ–FunASRè¯­éŸ³è¯†åˆ«å™¨...")

        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            return False

        # è®¾ç½®ç¯å¢ƒ
        self.setup_environment()

        # åŠ è½½æ¨¡å‹
        if not self._load_model():
            return False

        self._is_initialized = True
        logger.info("âœ… FunASRè¯­éŸ³è¯†åˆ«å™¨åˆå§‹åŒ–å®Œæˆ")
        return True

    def _load_model(self) -> bool:
        """åŠ è½½FunASRæ¨¡å‹"""
        if self._model_loaded:
            return True

        if not FUNASR_AVAILABLE:
            logger.error("âŒ FunASRä¸å¯ç”¨")
            return False

        logger.info(f"ğŸ“¦ åŠ è½½FunASRæ¨¡å‹: {self.model_path}")
        start_time = time.time()

        try:
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            if not os.path.exists(self.model_path):
                logger.error(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
                return False

            # åŠ è½½æ¨¡å‹
            self._model = AutoModel(
                model=self.funasr_config.model_path,
                device=self.funasr_config.device,
                trust_remote_code=self.funasr_config.trust_remote_code,
                disable_update=self.funasr_config.disable_update
            )

            self._model_loaded = True
            self._model_load_time = time.time() - start_time

            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è€—æ—¶: {self._model_load_time:.2f}ç§’)")
            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False

    def unload_model(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜"""
        if self._model:
            self._model = None
            self._model_loaded = False
            import gc
            gc.collect()
            logger.info("ğŸ§¹ æ¨¡å‹å·²å¸è½½")

    @contextmanager
    def _audio_stream(self):
        """éŸ³é¢‘æµä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if not PYAUDIO_AVAILABLE:
            raise RuntimeError("PyAudioä¸å¯ç”¨")

        p = pyaudio.PyAudio()
        stream = None

        try:
            # è·å–é»˜è®¤éŸ³é¢‘è®¾å¤‡
            default_device = p.get_default_input_device_info()
            logger.info(f"ğŸ¤ ä½¿ç”¨éŸ³é¢‘è®¾å¤‡: {default_device['name']}")

            # æ‰“å¼€éŸ³é¢‘æµ
            stream = p.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=self.chunk_size,
                start=True
            )

            logger.info("ğŸ§ éŸ³é¢‘æµåˆ›å»ºæˆåŠŸ")
            yield stream

        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘æµåˆ›å»ºå¤±è´¥: {e}")
            raise
        finally:
            if stream:
                if stream.is_active():
                    stream.stop_stream()
                stream.close()
            p.terminate()

    def _detect_vad(self, audio_data: np.ndarray, current_time: float) -> Tuple[bool, Optional[str]]:
        """
        VADè¯­éŸ³æ´»åŠ¨æ£€æµ‹

        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            current_time: å½“å‰æ—¶é—´

        Returns:
            (is_speech, event_type): æ˜¯å¦æœ‰è¯­éŸ³å’Œäº‹ä»¶ç±»å‹
        """
        # è®¡ç®—éŸ³é¢‘èƒ½é‡
        energy = np.sqrt(np.mean(audio_data ** 2))

        # ç®€å•çš„èƒ½é‡é˜ˆå€¼VAD
        is_speech = energy > self.vad_config.energy_threshold

        event_type = None
        if is_speech:
            if not hasattr(self, '_speech_detected') or not self._speech_detected:
                event_type = "speech_start"
                self._speech_detected = True
                self._speech_start_time = current_time
        else:
            if hasattr(self, '_speech_detected') and self._speech_detected:
                silence_duration = current_time - getattr(self, '_last_speech_time', current_time)
                if silence_duration >= self.vad_config.min_silence_duration:
                    event_type = "speech_end"
                    self._speech_detected = False

        if is_speech:
            self._last_speech_time = current_time

        return is_speech, event_type

    def _process_audio_chunk(self, audio_data: np.ndarray, current_time: float):
        """
        å¤„ç†éŸ³é¢‘å—

        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            current_time: å½“å‰æ—¶é—´
        """
        # æ·»åŠ åˆ°éŸ³é¢‘ç¼“å†²åŒº
        self._audio_buffer.extend(audio_data)

        # VADæ£€æµ‹
        is_speech, vad_event = self._detect_vad(audio_data, current_time)

        if vad_event and self._on_vad_event:
            self._on_vad_event(vad_event, {
                'time': current_time,
                'energy': np.sqrt(np.mean(audio_data ** 2))
            })

        # å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³ï¼Œæ·»åŠ åˆ°è¯­éŸ³ç¼“å†²åŒº
        if is_speech:
            self._speech_buffer.extend(audio_data)

            # å®šæœŸè¿›è¡Œæµå¼è¯†åˆ«
            if len(self._speech_buffer) >= self.sample_rate * 1:  # 1ç§’éŸ³é¢‘
                self._perform_streaming_recognition()
        else:
            # å¦‚æœé™éŸ³æ—¶é—´è¶³å¤Ÿé•¿ä¸”æœ‰è¯­éŸ³ç¼“å†²åŒºï¼Œè¿›è¡Œæœ€ç»ˆè¯†åˆ«
            if (len(self._speech_buffer) > 0 and
                hasattr(self, '_last_speech_time') and
                current_time - self._last_speech_time >= self.vad_config.min_silence_duration):

                if len(self._speech_buffer) >= self.sample_rate * self.vad_config.min_speech_duration:
                    self._perform_final_recognition()

    def _perform_streaming_recognition(self):
        """æ‰§è¡Œæµå¼è¯†åˆ«"""
        if not self._model or not self._model_loaded:
            return

        try:
            # å–æœ€è¿‘çš„éŸ³é¢‘æ•°æ®è¿›è¡Œè¯†åˆ«
            audio_array = np.array(list(self._speech_buffer))

            result = self._model.generate(
                input=audio_array,
                cache=self._funasr_cache,
                is_final=False,
                chunk_size=self.funasr_config.chunk_size,
                encoder_chunk_look_back=self.funasr_config.encoder_chunk_look_back,
                decoder_chunk_look_back=self.funasr_config.decoder_chunk_look_back
            )

            if result and isinstance(result, list) and len(result) > 0:
                text = result[0].get("text", "").strip()
                if text and text != self._current_text:
                    self._current_text = text
                    self._partial_results.append(text)

                    # è§¦å‘éƒ¨åˆ†ç»“æœå›è°ƒ
                    if self._on_partial_result:
                        self._on_partial_result(text)

                    if not self.silent_mode:
                        logger.info(f"ğŸ—£ï¸ æµå¼è¯†åˆ«: '{text}'")

        except Exception as e:
            logger.debug(f"æµå¼è¯†åˆ«å¼‚å¸¸: {e}")

    def _perform_final_recognition(self):
        """æ‰§è¡Œæœ€ç»ˆè¯†åˆ«"""
        if not self._model or not self._model_loaded or not self._speech_buffer:
            return

        try:
            start_time = time.time()

            audio_array = np.array(list(self._speech_buffer))

            result = self._model.generate(
                input=audio_array,
                cache=self._funasr_cache,
                is_final=True,
                chunk_size=self.funasr_config.chunk_size,
                encoder_chunk_look_back=self.funasr_config.encoder_chunk_look_back,
                decoder_chunk_look_back=self.funasr_config.decoder_chunk_look_back
            )

            processing_time = time.time() - start_time

            if result and isinstance(result, list) and len(result) > 0:
                text = result[0].get("text", "").strip()
                if text:
                    # åˆ›å»ºè¯†åˆ«ç»“æœ
                    recognition_result = RecognitionResult(
                        text=text,
                        partial_results=self._partial_results.copy(),
                        confidence=0.9,  # FunASRæš‚ä¸æä¾›ç½®ä¿¡åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        duration=len(self._speech_buffer) / self.sample_rate,
                        timestamp=time.time(),
                        audio_buffer=self._speech_buffer.copy()
                    )

                    self._final_results.append(recognition_result)
                    self.stats['total_recognitions'] += 1
                    self.stats['successful_recognitions'] += 1
                    self.stats['total_processing_time'] += processing_time

                    # è§¦å‘æœ€ç»ˆç»“æœå›è°ƒ
                    if self._on_final_result:
                        self._on_final_result(recognition_result)

                    if not self.silent_mode:
                        logger.info(f"âœ… æœ€ç»ˆè¯†åˆ«: '{text}' (è€—æ—¶: {processing_time:.3f}s)")

        except Exception as e:
            logger.error(f"æœ€ç»ˆè¯†åˆ«å¼‚å¸¸: {e}")
        finally:
            # æ¸…ç©ºè¯­éŸ³ç¼“å†²åŒº
            self._speech_buffer = []
            self._current_text = ""

    def recognize_speech(self, duration: int = 10,
                        real_time_display: bool = True) -> RecognitionResult:
        """
        è¯†åˆ«è¯­éŸ³ï¼ˆåŒæ­¥æ¨¡å¼ï¼‰

        Args:
            duration: è¯†åˆ«æ—¶é•¿ï¼ˆç§’ï¼‰
            real_time_display: æ˜¯å¦å®æ—¶æ˜¾ç¤ºè¯†åˆ«ç»“æœ

        Returns:
            RecognitionResult: è¯†åˆ«ç»“æœ
        """
        if not self._is_initialized:
            if not self.initialize():
                raise RuntimeError("åˆå§‹åŒ–å¤±è´¥")

        if not self.silent_mode:
            logger.info(f"ğŸ™ï¸ å¼€å§‹è¯­éŸ³è¯†åˆ«ï¼Œæ—¶é•¿: {duration}ç§’")

        # é‡ç½®çŠ¶æ€
        self._stop_event.clear()
        self._audio_buffer.clear()
        self._speech_buffer = []
        self._current_text = ""
        self._partial_results = []

        start_time = time.time()

        try:
            with self._audio_stream() as stream:
                while time.time() - start_time < duration and not self._stop_event.is_set():
                    try:
                        # è¯»å–éŸ³é¢‘æ•°æ®
                        data = stream.read(self.chunk_size, exception_on_overflow=False)
                        current_time = time.time() - start_time

                        # è½¬æ¢ä¸ºnumpyæ•°ç»„
                        audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0

                        # å¤„ç†éŸ³é¢‘
                        self._process_audio_chunk(audio_data, current_time)

                        # å®æ—¶æ˜¾ç¤º
                        if real_time_display and self._current_text:
                            remaining = duration - current_time
                            print(f"\rğŸ—£ï¸ è¯†åˆ«ä¸­: '{self._current_text}' | å‰©ä½™: {remaining:.1f}s",
                                 end="", flush=True)

                    except Exception as e:
                        logger.error(f"éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
                        continue

                # å¤„ç†æœ€åçš„éŸ³é¢‘
                if self._speech_buffer:
                    self._perform_final_recognition()

        except KeyboardInterrupt:
            logger.info("â¹ï¸ è¯†åˆ«è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            logger.error(f"è¯†åˆ«è¿‡ç¨‹å‡ºé”™: {e}")
            raise

        # è¿”å›æœ€ç»ˆç»“æœ
        if self._final_results:
            final_result = self._final_results[-1]
            print(f"\nâœ… è¯†åˆ«å®Œæˆ: '{final_result.text}'")
            return final_result
        else:
            # å¦‚æœæ²¡æœ‰æœ€ç»ˆç»“æœï¼Œä½¿ç”¨éƒ¨åˆ†ç»“æœ
            if self._partial_results:
                text = self._partial_results[-1] if self._partial_results else ""
                result = RecognitionResult(
                    text=text,
                    partial_results=self._partial_results,
                    confidence=0.5,
                    duration=duration,
                    timestamp=time.time(),
                    audio_buffer=[]
                )
                print(f"\nâš ï¸ æ— æœ€ç»ˆç»“æœï¼Œè¿”å›éƒ¨åˆ†ç»“æœ: '{text}'")
                return result
            else:
                logger.warning("âš ï¸ æœªè¯†åˆ«åˆ°ä»»ä½•è¯­éŸ³å†…å®¹")
                return RecognitionResult(
                    text="",
                    partial_results=[],
                    confidence=0.0,
                    duration=0.0,
                    timestamp=time.time(),
                    audio_buffer=[]
                )

    def start_continuous_recognition(self):
        """å¼€å§‹è¿ç»­è¯†åˆ«ï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰"""
        if not self._is_initialized:
            if not self.initialize():
                raise RuntimeError("åˆå§‹åŒ–å¤±è´¥")

        if self._is_running:
            logger.warning("âš ï¸ è¿ç»­è¯†åˆ«å·²åœ¨è¿è¡Œ")
            return

        logger.info("ğŸ”„ å¼€å§‹è¿ç»­è¯†åˆ«æ¨¡å¼")
        self._is_running = True
        self._stop_event.clear()

        def recognition_thread():
            try:
                with self._audio_stream() as stream:
                    while self._is_running and not self._stop_event.is_set():
                        try:
                            data = stream.read(self.chunk_size, exception_on_overflow=False)
                            current_time = time.time()

                            audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
                            self._process_audio_chunk(audio_data, current_time)

                        except Exception as e:
                            logger.error(f"è¿ç»­è¯†åˆ«é”™è¯¯: {e}")
                            continue

            except Exception as e:
                logger.error(f"è¿ç»­è¯†åˆ«çº¿ç¨‹å¼‚å¸¸: {e}")
            finally:
                self._is_running = False
                logger.info("ğŸ”„ è¿ç»­è¯†åˆ«çº¿ç¨‹ç»“æŸ")

        # å¯åŠ¨è¯†åˆ«çº¿ç¨‹
        thread = threading.Thread(target=recognition_thread, daemon=True)
        thread.start()

        return thread

    def stop_recognition(self):
        """åœæ­¢è¯†åˆ«"""
        logger.info("â¹ï¸ åœæ­¢è¯†åˆ«")
        self._stop_event.set()
        self._is_running = False

        # å¤„ç†æœ€åçš„éŸ³é¢‘
        if self._speech_buffer:
            self._perform_final_recognition()

    def get_status(self) -> Dict[str, Any]:
        """è·å–è¯†åˆ«å™¨çŠ¶æ€"""
        return {
            'initialized': self._is_initialized,
            'model_loaded': self._model_loaded,
            'model_path': self.model_path,
            'device': self.funasr_config.device,
            'running': self._is_running,
            'stats': self.stats.copy(),
            'model_load_time': self._model_load_time,
            'dependencies': {
                'funasr': FUNASR_AVAILABLE,
                'pyaudio': PYAUDIO_AVAILABLE,
                'numpy': NUMPY_AVAILABLE
            }
        }

    def configure_vad(self, **kwargs):
        """é…ç½®VADå‚æ•°"""
        for key, value in kwargs.items():
            if hasattr(self.vad_config, key):
                setattr(self.vad_config, key, value)
                logger.info(f"ğŸ”§ VADé…ç½®æ›´æ–°: {key} = {value}")
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥çš„VADå‚æ•°: {key}")

    def configure_funasr(self, **kwargs):
        """é…ç½®FunASRå‚æ•°"""
        for key, value in kwargs.items():
            if hasattr(self.funasr_config, key):
                setattr(self.funasr_config, key, value)
                logger.info(f"ğŸ”§ FunASRé…ç½®æ›´æ–°: {key} = {value}")
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥çš„FunASRå‚æ•°: {key}")

    def __del__(self):
        """ææ„å‡½æ•°"""
        try:
            self.stop_recognition()
            self.unload_model()
        except:
            pass

# ä¾¿æ·å‡½æ•°
def create_recognizer(model_path: Optional[str] = None,
                     device: str = "cpu") -> FunASRVoiceRecognizer:
    """
    åˆ›å»ºå¹¶åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        device: è®¾å¤‡ç±»å‹

    Returns:
        FunASRVoiceRecognizer: å·²åˆå§‹åŒ–çš„è¯†åˆ«å™¨
    """
    recognizer = FunASRVoiceRecognizer(model_path=model_path, device=device)
    if not recognizer.initialize():
        raise RuntimeError("è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥")
    return recognizer

def quick_recognize(duration: int = 10,
                   model_path: Optional[str] = None) -> str:
    """
    å¿«é€Ÿè¯­éŸ³è¯†åˆ«

    Args:
        duration: è¯†åˆ«æ—¶é•¿
        model_path: æ¨¡å‹è·¯å¾„

    Returns:
        str: è¯†åˆ«ç»“æœæ–‡æœ¬
    """
    recognizer = create_recognizer(model_path=model_path)
    result = recognizer.recognize_speech(duration=duration)
    return result.text

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ¯ FunASRè¯­éŸ³è¯†åˆ«æ¨¡å—æµ‹è¯•")
    print("=" * 50)

    # åˆ›å»ºè¯†åˆ«å™¨
    recognizer = FunASRVoiceRecognizer()

    # è®¾ç½®å›è°ƒ
    def on_partial(text):
        print(f"ğŸ—£ï¸ å®æ—¶: {text}")

    def on_final(result):
        print(f"âœ… æœ€ç»ˆ: {result.text}")

    def on_vad(event, data):
        print(f"ğŸ¯ VAD: {event}")

    recognizer.set_callbacks(on_partial, on_final, on_vad)

    # åˆå§‹åŒ–
    if recognizer.initialize():
        print("âœ… è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"ğŸ“Š çŠ¶æ€: {recognizer.get_status()}")

        # è¿›è¡Œè¯†åˆ«æµ‹è¯•
        try:
            result = recognizer.recognize_speech(duration=15)
            print(f"\nğŸ¯ æœ€ç»ˆè¯†åˆ«ç»“æœ: '{result.text}'")
            print(f"ğŸ“Š è¯†åˆ«æ—¶é•¿: {result.duration:.2f}ç§’")
            print(f"ğŸ“Š éƒ¨åˆ†ç»“æœæ•°: {len(result.partial_results)}")
        except KeyboardInterrupt:
            print("\nâ¹ï¸ æµ‹è¯•è¢«ä¸­æ–­")
    else:
        print("âŒ è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥")